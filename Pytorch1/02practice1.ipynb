{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'1.6.0'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import torch.nn as nn\n",
    "\n",
    "torch.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 0.0000e+00, -1.0842e-19,  0.0000e+00],\n",
      "        [-1.0842e-19,  9.8091e-45,  0.0000e+00]])\n",
      "tensor([[0., 0., 0.],\n",
      "        [0., 0., 0.]])\n"
     ]
    }
   ],
   "source": [
    "# Create uninitialized tensor\n",
    "x = torch.FloatTensor(2,3)\n",
    "print(x)\n",
    "#Initialize to zeros\n",
    "x.zero_()\n",
    "print(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-0.1115,  0.1204, -0.3696],\n",
      "        [-0.2404, -1.1969,  0.2093]])\n",
      "[[-0.11146712  0.12036294 -0.3696345 ]\n",
      " [-0.24041797 -1.1969243   0.20926936]]\n"
     ]
    }
   ],
   "source": [
    "# Create random tensor (seed for repeatability)\n",
    "torch.manual_seed(123)\n",
    "x=torch.randn(2,3)\n",
    "print(x)\n",
    "# export to numpy array\n",
    "x_np = x.numpy()\n",
    "print(x_np)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(torch.eye(3))\n",
    "print(torch.ones(3,3))\n",
    "print(torch.zeros(2,3)) # make matrix\n",
    "print(torch.arange(1,3)) # make array"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "All tensor have a size and type."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([3, 4]) torch.FloatTensor\n"
     ]
    }
   ],
   "source": [
    "x=torch.FloatTensor(3,4)\n",
    "print(x.size(), x.type())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Math, Linear Algebra, and Indexing (review )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([1., 2., 3., 4.])\n",
      "tensor(10.)\n",
      "tensor([ 2.7183,  7.3891, 20.0855, 54.5982])\n",
      "tensor(84.7910)\n",
      "tensor(2.5000)\n",
      "\n",
      "tensor([ 2,  7, 20, 54], dtype=torch.int32)\n"
     ]
    }
   ],
   "source": [
    "x = torch.arange(0., 5.)\n",
    "y = torch.exp(x)\n",
    "xnt = y.type(dtype=torch.IntTensor)\n",
    "print(x)\n",
    "print(torch.sum(x))\n",
    "print(torch.exp(x))\n",
    "print(torch.sum(torch.exp(x)))\n",
    "print(torch.mean(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "ename": "AssertionError",
     "evalue": "Torch not compiled with CUDA enabled",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-23-1a729973c03d>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrand\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;31m# copy to GPU\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0;31m# copy back to CPU\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0mz\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcpu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/trchenv3.7/lib/python3.7/site-packages/torch/cuda/__init__.py\u001b[0m in \u001b[0;36m_lazy_init\u001b[0;34m()\u001b[0m\n\u001b[1;32m    194\u001b[0m             raise RuntimeError(\n\u001b[1;32m    195\u001b[0m                 \"Cannot re-initialize CUDA in forked subprocess. \" + msg)\n\u001b[0;32m--> 196\u001b[0;31m         \u001b[0m_check_driver\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    197\u001b[0m         \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_C\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_cuda_init\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    198\u001b[0m         \u001b[0m_cudart\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_load_cudart\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/trchenv3.7/lib/python3.7/site-packages/torch/cuda/__init__.py\u001b[0m in \u001b[0;36m_check_driver\u001b[0;34m()\u001b[0m\n\u001b[1;32m     92\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0m_check_driver\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     93\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_C\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'_cuda_isDriverSufficient'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 94\u001b[0;31m         \u001b[0;32mraise\u001b[0m \u001b[0mAssertionError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Torch not compiled with CUDA enabled\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     95\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_C\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_cuda_isDriverSufficient\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     96\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_C\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_cuda_getDriverVersion\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAssertionError\u001b[0m: Torch not compiled with CUDA enabled"
     ]
    }
   ],
   "source": [
    "# create a tensor\n",
    "x = torch.rand(3,2)\n",
    "# copy to GPU\n",
    "y = x.cuda()\n",
    "# copy back to CPU\n",
    "z = y.cpu()\n",
    "# get CPU tensor as numpy array\n",
    "# cannot get GPU tensor as numpy array directly\n",
    "try:\n",
    "    y.numpy()\n",
    "except RuntimeError as e:\n",
    "    print(e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "ename": "AssertionError",
     "evalue": "Torch not compiled with CUDA enabled",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-24-360dae2881bb>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrand\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m5\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# CPU tensor\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrand\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m5\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m4\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# GPU tensor\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# Operation between CPU and GPU fails\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mexcept\u001b[0m \u001b[0mTypeError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/trchenv3.7/lib/python3.7/site-packages/torch/cuda/__init__.py\u001b[0m in \u001b[0;36m_lazy_init\u001b[0;34m()\u001b[0m\n\u001b[1;32m    194\u001b[0m             raise RuntimeError(\n\u001b[1;32m    195\u001b[0m                 \"Cannot re-initialize CUDA in forked subprocess. \" + msg)\n\u001b[0;32m--> 196\u001b[0;31m         \u001b[0m_check_driver\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    197\u001b[0m         \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_C\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_cuda_init\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    198\u001b[0m         \u001b[0m_cudart\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_load_cudart\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/trchenv3.7/lib/python3.7/site-packages/torch/cuda/__init__.py\u001b[0m in \u001b[0;36m_check_driver\u001b[0;34m()\u001b[0m\n\u001b[1;32m     92\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0m_check_driver\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     93\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_C\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'_cuda_isDriverSufficient'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 94\u001b[0;31m         \u001b[0;32mraise\u001b[0m \u001b[0mAssertionError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Torch not compiled with CUDA enabled\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     95\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_C\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_cuda_isDriverSufficient\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     96\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_C\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_cuda_getDriverVersion\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAssertionError\u001b[0m: Torch not compiled with CUDA enabled"
     ]
    }
   ],
   "source": [
    "x = torch.rand(3,5)  # CPU tensor\n",
    "y = torch.rand(5,4).cuda()  # GPU tensor\n",
    "try:\n",
    "    torch.mm(x,y)  # Operation between CPU and GPU fails\n",
    "except TypeError as e:\n",
    "    print(e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'nvcc' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-26-7fcf4273bd03>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mnvcc\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0mversion\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'nvcc' is not defined"
     ]
    }
   ],
   "source": [
    "nvcc --version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.4805, 0.1896],\n",
      "        [0.0012, 0.0364],\n",
      "        [0.8589, 0.2808]])\n"
     ]
    }
   ],
   "source": [
    "# Put tensor on CUDA if available\n",
    "x = torch.rand(3,2)\n",
    "if torch.cuda.is_available():\n",
    "    x = x.cuda()\n",
    "    print(x, x.dtype)\n",
    "    \n",
    "# Do some calculations\n",
    "y = x ** 2 \n",
    "print(y)\n",
    "\n",
    "# Copy to CPU if on GPU\n",
    "if y.is_cuda:\n",
    "    y = y.cpu()\n",
    "    print(y, y.dtype)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "x1 = torch.rand(3, 2)\n",
    "x2 = x1.new(1,2)\n",
    "print(x2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.1610, 0.5661],\n",
      "        [0.2441, 0.7232],\n",
      "        [0.4741, 0.2356]])\n",
      "tensor([[0., 2.]])\n"
     ]
    }
   ],
   "source": [
    "x1 = torch.rand(3, 2) \n",
    "x2 = x1.new(1, 2) \n",
    "print(x1)\n",
    "print(x2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU : 467.3991639999713ms\n"
     ]
    }
   ],
   "source": [
    "from timeit import timeit\n",
    "# Create random data\n",
    "x = torch.rand(1000, 64)\n",
    "y = torch.rand(64, 32)\n",
    "num = 10000 #number of iterations.\n",
    "\n",
    "def square():\n",
    "    z = torch.mm(x, y)\n",
    "print('CPU : {}ms'.format(timeit(square, number = num)*1000))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1., 1., 1., 1.],\n",
      "        [1., 1., 1., 1.]], requires_grad=True)\n",
      "1 tensor([[1., 1., 1., 1.],\n",
      "        [1., 1., 1., 1.]], grad_fn=<PowBackward0>)\n",
      "2 tensor([[1., 1., 1., 1.],\n",
      "        [1., 1., 1., 1.]], grad_fn=<PowBackward0>)\n",
      "tensor([[4., 4., 4., 4.],\n",
      "        [4., 4., 4., 4.]])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/KangDaeWon/anaconda3/envs/trchenv3.7/lib/python3.7/site-packages/ipykernel_launcher.py:3: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  This is separate from the ipykernel package so we can avoid doing imports until\n"
     ]
    }
   ],
   "source": [
    "# Create differentiable tensor\n",
    "# x = torch.tensor(torch.arange(0,4), requires_grad=False)\n",
    "x = torch.tensor(torch.ones(2,4), requires_grad=True)\n",
    "# print(x.dtype)\n",
    "# x.grad.data.zero_()\n",
    "print(x)\n",
    "torch.sum(x**2).backward()\n",
    "y = x**2\n",
    "print('1', y)\n",
    "y.sum().backward()\n",
    "print('2', y)\n",
    "print(x.grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 tensor([0, 1, 2, 3]) \n",
      "1 tensor([0, 1, 4, 9]) \n",
      "2 tensor([ 0,  1, 16, 81])\n",
      "tensor([ 0,  1, 16, 81])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/KangDaeWon/anaconda3/envs/trchenv3.7/lib/python3.7/site-packages/ipykernel_launcher.py:1: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  \"\"\"Entry point for launching an IPython kernel.\n"
     ]
    }
   ],
   "source": [
    "x = torch.tensor(torch.arange(0,4), requires_grad=False)\n",
    "y = x**2\n",
    "z = y**2\n",
    "print('0', x,'\\n1', y, '\\n2', z)\n",
    "z.detach().numpy()\n",
    "print(z)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "Expected object of scalar type Long but got scalar type Float for argument #3 'mat2' in call to _th_addmm_out",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-27-8161476a6fbd>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m32\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mnet\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mLinear\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m32\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m10\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnet\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/trchenv3.7/lib/python3.7/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    720\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    721\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 722\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    723\u001b[0m         for hook in itertools.chain(\n\u001b[1;32m    724\u001b[0m                 \u001b[0m_global_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/trchenv3.7/lib/python3.7/site-packages/torch/nn/modules/linear.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m     89\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     90\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 91\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlinear\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbias\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     92\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     93\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mextra_repr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/trchenv3.7/lib/python3.7/site-packages/torch/nn/functional.py\u001b[0m in \u001b[0;36mlinear\u001b[0;34m(input, weight, bias)\u001b[0m\n\u001b[1;32m   1674\u001b[0m         \u001b[0mret\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maddmm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbias\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweight\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1675\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1676\u001b[0;31m         \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmatmul\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1677\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mbias\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1678\u001b[0m             \u001b[0moutput\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mbias\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Expected object of scalar type Long but got scalar type Float for argument #3 'mat2' in call to _th_addmm_out"
     ]
    }
   ],
   "source": [
    "x = torch.arange(0, 32)\n",
    "net = torch.nn.Linear(32, 10)\n",
    "y = net(x)\n",
    "print(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sequential(\n",
      "  (0): Linear(in_features=32, out_features=128, bias=True)\n",
      "  (1): Sigmoid()\n",
      "  (2): Linear(in_features=128, out_features=10, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "# create a simple seqyential network(nn.Module; object) from layers (other 'nn.Module'; object)\n",
    "# Here a MLP with 2 layers and sigmoid activation.\n",
    "net = torch.nn.Sequential(\n",
    "    torch.nn.Linear(32, 128),\n",
    "    torch.nn.Sigmoid(),\n",
    "    torch.nn.Linear(128, 10))\n",
    "print(net)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MyNetwork(\n",
      "  (layer1): Linear(in_features=32, out_features=128, bias=True)\n",
      "  (layer2): Sigmoid()\n",
      "  (layer3): Linear(in_features=128, out_features=10, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "# Create a more customizable network module (equibalent here)\n",
    "class MyNetwork(torch.nn.Module):\n",
    "    def __init__(self, inputSize, hiddenSize, outputSize):\n",
    "        super().__init__()\n",
    "        self.layer1 = torch.nn.Linear(inputSize, hiddenSize)\n",
    "        self.layer2 = torch.nn.Sigmoid()\n",
    "        self.layer3 = torch.nn.Linear(hiddenSize, outputSize)\n",
    "        \n",
    "    def forward(self, inputVal):\n",
    "        h = inputVal\n",
    "        h = self.layer1(h)\n",
    "#         print('layer1', h)\n",
    "        h = self.layer2(h)\n",
    "#         print('layer2', h)\n",
    "        h = self.layer3(h)\n",
    "#         print('layer3', h)\n",
    "        return h\n",
    "\n",
    "net = MyNetwork(32, 128, 10)\n",
    "print(net)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parameter containing:\n",
      "tensor([[-0.0134, -0.2918, -0.1799,  ..., -1.0561,  0.3473,  1.2354],\n",
      "        [ 0.5572, -0.8590, -0.6649,  ..., -0.5015,  0.2164,  0.9878],\n",
      "        [ 0.4380,  0.8587, -1.5056,  ..., -0.5404, -1.0242,  0.7898],\n",
      "        ...,\n",
      "        [ 0.6764,  0.7673,  0.6491,  ..., -0.7212,  1.2370,  0.9538],\n",
      "        [ 1.0513,  2.3330,  0.5287,  ...,  0.8888,  0.9517, -0.3849],\n",
      "        [ 1.0079, -0.0903, -0.5072,  ..., -0.5454,  1.1342,  0.2860]],\n",
      "       requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([-0.3493, -0.6155,  0.0049,  0.7552,  0.6222,  1.0016, -0.9386, -0.5945,\n",
      "         1.0814, -0.5820, -1.0319, -0.5961, -0.2457, -0.9504, -0.0978, -1.0557,\n",
      "         0.5539,  0.1254,  0.2426,  0.4270,  0.0608, -0.6844,  1.4274,  0.6769,\n",
      "         0.2477,  0.3832, -0.8889,  1.0568,  2.2727,  0.8648,  2.4866, -0.4655,\n",
      "         1.1351,  0.1416, -0.5216,  0.5953,  2.0680,  0.2783, -0.1957, -0.5393,\n",
      "        -1.5547, -0.5468, -1.0406,  0.8948, -0.0853, -2.1782,  0.7632, -1.2043,\n",
      "         0.2252,  0.7780,  0.4191, -0.7924, -1.5221,  1.1313, -1.0727,  0.8086,\n",
      "        -0.6333,  0.4385,  0.4097,  0.1853, -1.5874, -0.5793, -1.7930, -0.1635,\n",
      "        -2.0575, -0.8369, -1.8619, -1.8326, -0.8675,  0.9605,  2.1945,  1.3753,\n",
      "        -0.3696, -0.3117, -0.0807,  1.0746,  0.3805, -1.3371, -0.8462, -1.0475,\n",
      "         2.2155,  1.2462, -1.1097,  0.5094, -0.6024,  0.4878, -0.6889,  0.3260,\n",
      "         2.8870,  1.3522, -0.6022,  0.9206, -0.0711, -0.8597, -0.6768,  2.0321,\n",
      "         0.2001, -1.8782,  0.1694, -0.1125, -0.2102, -1.3663,  0.4006, -0.7406,\n",
      "        -0.1852,  0.7221, -0.0426, -0.4076,  0.0165,  0.5914,  2.0773, -1.1046,\n",
      "         0.0617, -0.2884,  0.5618, -0.4819, -0.7924,  0.2792, -0.5986,  0.1598,\n",
      "         0.2757, -0.2646,  1.4107,  1.3695,  0.8629, -1.0115,  0.2756,  0.4043],\n",
      "       requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([[-0.8188, -0.4469,  1.0158,  ..., -1.1463, -0.1382,  0.4178],\n",
      "        [ 0.9365, -0.1327,  0.4890,  ...,  1.9832,  0.4518,  0.5847],\n",
      "        [-0.8164,  1.6870, -0.5099,  ..., -1.2278, -0.3468, -0.2461],\n",
      "        ...,\n",
      "        [-2.1729, -1.7083, -0.2703,  ..., -0.4396,  0.2844, -0.1891],\n",
      "        [-1.6675,  0.6072,  1.1070,  ..., -1.0316, -0.2351,  0.2045],\n",
      "        [ 0.6210, -1.4868,  1.9954,  ..., -0.0417,  0.8693,  0.3018]],\n",
      "       requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([-0.5376, -0.8201, -0.9808, -0.2789, -1.1953, -0.5872,  0.2561, -0.8231,\n",
      "        -1.2844, -0.3888], requires_grad=True)\n"
     ]
    }
   ],
   "source": [
    "for param in net.parameters():\n",
    "    print(param)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MyNetworkWithParmas()\n"
     ]
    }
   ],
   "source": [
    "class MyNetworkWithParmas(nn.Module):\n",
    "    def __init__(self, inputSize, hiddenSize, outputSize):\n",
    "        super(MyNetworkWithParmas, self).__init__()\n",
    "        self.layer1_weights = nn.Parameter(torch.randn(inputSize, hiddenSize))\n",
    "        self.layer1_bias = nn.Parameter(torch.randn(hiddenSize))\n",
    "        self.layer2_weights = nn.Parameter(torch.randn(hiddenSize, outputSize))\n",
    "        self.layer2_bias = nn.Parameter(torch.randn(outputSize))\n",
    "    def forward(self, x):\n",
    "        h1 = torch.matmul(x, self.layer1_weights) + self.layer1_bias\n",
    "        h1_act = torch.max(h1, torch.zeros(h1.size())) # ReLU\n",
    "#         print('activatefunction', h1_act)\n",
    "        output = torch.matmul(hl_act, self.layer2_weights) + self.layer2_bias\n",
    "        return output\n",
    "    \n",
    "net = MyNetworkWithParmas(32, 128, 10)\n",
    "print(net)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "net = MyNetwork(32, 128, 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(2.4306, grad_fn=<NllLossBackward>)\n"
     ]
    }
   ],
   "source": [
    "x = torch.tensor([np.arange(32), np.zeros(32), np.ones(32)]).float()\n",
    "y = torch.tensor([0,3,9])\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "output = net(x)\n",
    "loss = criterion(output, y)\n",
    "print(loss)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/KangDaeWon/anaconda3/envs/trchenv3.7/lib/python3.7/site-packages/ipykernel_launcher.py:5: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  \"\"\"\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor(2.4306, grad_fn=<NllLossBackward>)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# equivalent\n",
    "criterion2 = nn.NLLLoss()\n",
    "sf = nn.LogSoftmax()\n",
    "output = net(x)\n",
    "loss = criterion2(sf(output), y)\n",
    "loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-0.0154, -0.0153, -0.0152,  ..., -0.0127, -0.0126, -0.0125],\n",
      "        [ 0.0180,  0.0180,  0.0180,  ...,  0.0180,  0.0180,  0.0180],\n",
      "        [-0.0135, -0.0206, -0.0277,  ..., -0.2191, -0.2262, -0.2333],\n",
      "        ...,\n",
      "        [ 0.0174,  0.0131,  0.0089,  ..., -0.1065, -0.1107, -0.1150],\n",
      "        [-0.0052, -0.0052, -0.0052,  ..., -0.0051, -0.0051, -0.0051],\n",
      "        [ 0.0144,  0.0144,  0.0144,  ...,  0.0142,  0.0142,  0.0142]])\n",
      "tensor([-0.0336,  0.0271,  0.0025,  0.0085, -0.0094, -0.0125,  0.0183, -0.0241,\n",
      "        -0.0104, -0.0140, -0.0109, -0.0102,  0.0216, -0.0137,  0.0044, -0.0216,\n",
      "        -0.0278,  0.0141,  0.0200,  0.0034,  0.0185, -0.0081, -0.0167, -0.0099,\n",
      "        -0.0241, -0.0052,  0.0278,  0.0119, -0.0266,  0.0139,  0.0022, -0.0226,\n",
      "         0.0305, -0.0035, -0.0032, -0.0279,  0.0238, -0.0145,  0.0012,  0.0141,\n",
      "        -0.0076, -0.0043,  0.0227, -0.0162, -0.0045,  0.0183,  0.0107, -0.0054,\n",
      "         0.0108, -0.0098, -0.0283, -0.0170,  0.0244,  0.0088,  0.0158,  0.0196,\n",
      "         0.0016, -0.0035,  0.0082,  0.0321, -0.0058,  0.0215,  0.0205,  0.0064,\n",
      "         0.0120,  0.0051, -0.0161, -0.0030,  0.0120,  0.0050,  0.0116,  0.0089,\n",
      "         0.0169,  0.0124,  0.0125,  0.0045,  0.0025,  0.0055,  0.0215,  0.0122,\n",
      "         0.0080, -0.0019,  0.0249,  0.0074,  0.0193,  0.0034,  0.0211,  0.0206,\n",
      "         0.0011, -0.0023, -0.0203,  0.0129,  0.0128,  0.0103, -0.0013, -0.0259,\n",
      "         0.0116,  0.0168,  0.0220,  0.0244,  0.0118, -0.0112, -0.0127, -0.0107,\n",
      "         0.0113,  0.0024,  0.0300, -0.0039, -0.0122, -0.0169, -0.0004, -0.0015,\n",
      "        -0.0079,  0.0317, -0.0125, -0.0161, -0.0254,  0.0030, -0.0038,  0.0088,\n",
      "        -0.0075,  0.0044, -0.0070, -0.0127, -0.0059,  0.0236, -0.0130,  0.0105])\n",
      "tensor([[-0.8377,  0.0767, -0.0327,  ...,  0.0251,  0.0860,  0.0951],\n",
      "        [ 0.1196,  0.0638,  0.0769,  ...,  0.0802,  0.0716,  0.0789],\n",
      "        [ 0.2943,  0.1077,  0.1421,  ...,  0.1426,  0.1210,  0.1338],\n",
      "        ...,\n",
      "        [ 0.1308,  0.0528,  0.0684,  ...,  0.0695,  0.0596,  0.0662],\n",
      "        [ 0.3239,  0.1523,  0.1887,  ...,  0.1944,  0.1708,  0.1885],\n",
      "        [-0.2926, -0.2755, -0.3697,  ..., -0.4038, -0.3483, -0.4284]])\n",
      "tensor([-0.7569,  0.1876,  0.4097, -0.7752,  0.3491,  0.3112,  0.2543,  0.1878,\n",
      "         0.4864, -0.6539])\n"
     ]
    }
   ],
   "source": [
    "loss.backward()\n",
    "# Check that the parameters now have gradients.\n",
    "for param in net.parameters():\n",
    "    print(param.grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "layer1 tensor([[-5.1618e+00, -4.2152e+00, -8.4670e+00,  1.4110e+01,  4.8130e+00,\n",
      "         -6.6926e+00, -1.2622e+01,  1.4639e+01, -1.2471e+01, -6.9453e+00,\n",
      "          4.4591e+00,  8.3486e+00,  1.1296e+01, -3.0892e+00,  4.0617e+00,\n",
      "         -6.6144e+00, -2.4907e+00,  7.0176e+00, -1.0662e+00,  4.5190e+00,\n",
      "         -2.3595e+01,  3.5734e+00,  1.3358e+00,  4.4969e+00, -2.3691e+00,\n",
      "         -3.8123e-01, -5.0096e+00,  4.7195e+00, -1.5410e+01, -3.9049e+00,\n",
      "          4.9635e+00,  1.1775e+01, -7.1133e+00, -3.9520e+00, -8.7899e-01,\n",
      "          1.7951e+01,  6.2363e+00,  7.5335e+00,  1.3576e+01,  4.9625e+00,\n",
      "          6.7328e+00, -7.2595e+00,  1.0612e+01, -9.8447e+00,  1.9025e-01,\n",
      "         -1.2046e+01, -7.7834e+00, -6.0830e+00,  1.2308e+01, -7.1781e+00,\n",
      "         -1.8929e+01,  6.3393e+00, -1.1182e+01,  4.6790e-01, -4.3229e+00,\n",
      "          7.8729e+00, -1.5258e+01,  9.7835e+00,  3.8424e+00,  2.2110e+00,\n",
      "          6.3909e+00, -1.9023e+01,  1.4249e+01,  1.0482e+00,  1.2440e+01,\n",
      "         -7.8790e+00,  1.6814e+01, -2.5699e+01,  1.2589e+01, -2.0660e+01,\n",
      "          2.2361e+00, -7.6086e+00,  7.5920e+00,  2.7316e+00,  5.0562e+00,\n",
      "         -5.0219e+00,  1.7953e+01, -7.5878e+00,  7.8789e+00,  1.1470e+01,\n",
      "         -9.5968e-01, -9.6922e+00, -9.0786e+00, -1.0052e+01,  8.3738e+00,\n",
      "          1.7142e+01, -9.0326e+00,  1.3228e+01,  1.0658e+01,  9.1353e+00,\n",
      "          1.6991e+01,  1.3973e+01,  4.9652e+00, -1.5770e+01,  2.1958e+00,\n",
      "          2.0932e+00,  9.1460e-02, -6.0878e+00, -4.7965e+00,  1.8414e+00,\n",
      "          2.4282e+00, -1.7735e+00, -7.2456e+00,  3.4987e+00, -1.4040e+00,\n",
      "         -1.3718e+01,  1.2135e+01,  6.6168e+00, -6.1617e-01,  2.1049e-01,\n",
      "          2.0342e+01,  5.1593e+00,  2.2236e+01, -1.1709e+01,  1.3859e+01,\n",
      "         -1.3463e+01, -1.7843e+00, -9.2308e+00, -3.9392e+00,  2.6577e+00,\n",
      "          1.2801e+01,  7.4037e+00,  1.1784e+01,  5.7293e+00,  5.1478e+00,\n",
      "          6.3228e+00, -7.3992e+00,  4.2733e-01],\n",
      "        [-4.0159e-02, -9.9294e-02, -8.9895e-02, -1.6091e-02,  1.4256e-01,\n",
      "         -1.6360e-02,  4.8316e-02, -1.5389e-01, -1.0784e-01, -5.0343e-02,\n",
      "         -1.0425e-01,  9.3292e-02,  2.4054e-02,  7.4690e-02,  6.9636e-02,\n",
      "          8.0688e-02, -4.4123e-02,  1.4575e-01,  1.3394e-01,  8.0742e-02,\n",
      "         -2.8184e-02, -1.0012e-01,  1.2153e-01,  7.9604e-02, -2.8956e-02,\n",
      "         -1.0943e-01, -3.1854e-02, -9.7265e-02, -1.7223e-01,  1.3697e-01,\n",
      "         -6.9313e-02,  1.0456e-01,  5.0276e-02,  2.7027e-02, -2.3206e-02,\n",
      "          1.5111e-01,  7.8399e-03,  6.8676e-02,  3.5505e-02,  6.1230e-02,\n",
      "         -8.4925e-02,  1.6478e-01, -1.5812e-01, -2.4261e-02,  9.8777e-02,\n",
      "          7.2197e-02,  9.2127e-02,  2.0429e-02,  2.5663e-02,  1.2782e-01,\n",
      "         -5.5344e-02, -1.0498e-01, -2.6496e-02,  3.8555e-02, -1.0996e-01,\n",
      "         -5.6475e-02, -1.7242e-01, -8.2958e-02, -5.0914e-02,  1.5333e-01,\n",
      "         -1.5445e-01, -1.4422e-01,  4.1916e-02, -1.4901e-01, -4.8993e-02,\n",
      "         -1.0284e-01, -1.6944e-02, -1.5977e-01,  4.1540e-02,  7.2491e-02,\n",
      "          1.5028e-01, -6.0289e-02, -4.1806e-02, -3.8239e-02, -9.8579e-02,\n",
      "         -1.4725e-01, -1.5148e-01, -1.2111e-01,  1.0216e-01, -5.3280e-02,\n",
      "          2.4666e-02, -1.3911e-01,  9.4461e-02,  2.5234e-02, -1.0352e-01,\n",
      "         -1.2309e-01,  2.4462e-02,  7.3409e-02, -1.2050e-01, -5.6063e-02,\n",
      "          7.0540e-02, -9.1485e-02, -1.5976e-01, -7.4644e-02, -6.2973e-02,\n",
      "          1.4798e-01,  1.4651e-02, -6.7457e-02, -2.1890e-02,  8.9072e-02,\n",
      "         -9.6915e-03,  6.6886e-02,  7.6476e-02, -1.7036e-01,  1.7421e-01,\n",
      "          6.8963e-02,  2.0286e-02,  1.1301e-01,  4.0170e-02,  8.6716e-02,\n",
      "         -1.2721e-01,  2.9738e-02, -3.1544e-02, -1.3908e-02,  4.3627e-03,\n",
      "         -1.7392e-01, -1.2254e-01, -7.6004e-02,  1.7416e-01,  8.6714e-02,\n",
      "         -1.9331e-02,  1.6562e-02, -2.4129e-02,  6.1151e-02,  1.4085e-01,\n",
      "         -1.3116e-01, -5.3703e-02, -2.5768e-02],\n",
      "        [ 2.2405e-02, -3.8982e-01, -6.1869e-01,  5.0226e-01,  3.8613e-01,\n",
      "         -1.1434e-01, -3.7417e-01,  5.8687e-01, -5.1449e-01, -7.6573e-01,\n",
      "          3.3475e-02,  6.5186e-02,  4.9915e-01, -1.0044e-01,  2.4841e-01,\n",
      "         -5.3491e-01, -1.7133e-01,  4.2491e-01, -6.4686e-02, -4.1935e-02,\n",
      "         -1.0688e+00, -1.1936e-01,  2.0071e-02,  6.1194e-01,  1.9218e-01,\n",
      "          3.7566e-02, -3.2318e-01, -1.4143e-01, -8.9021e-01, -3.7517e-01,\n",
      "          2.9025e-01,  1.7541e-01, -2.4778e-01, -4.2591e-01, -1.4682e-01,\n",
      "          1.1580e+00,  3.8565e-02,  1.2448e-01,  5.4870e-01,  8.3577e-01,\n",
      "          1.1089e-01, -2.6464e-01,  2.0600e-01, -5.8538e-01, -3.8686e-02,\n",
      "         -6.2438e-01, -2.5229e-01, -7.7221e-02,  6.9347e-01,  2.1288e-01,\n",
      "         -1.5801e+00, -1.0036e-01, -6.2938e-01,  3.7787e-01, -9.7309e-02,\n",
      "          3.3324e-01, -7.7082e-01,  7.7159e-01, -1.3824e-01, -4.1009e-02,\n",
      "          1.4255e-02, -1.0448e+00,  8.5480e-01,  2.9040e-01,  7.5803e-01,\n",
      "         -6.0011e-01,  7.1517e-01, -1.9186e+00,  1.7959e-01, -1.0929e+00,\n",
      "         -3.7153e-02, -5.2127e-01,  5.3206e-02, -1.2899e-01,  8.4548e-02,\n",
      "         -6.4670e-01,  1.1487e-02,  4.8612e-02,  1.1133e+00,  9.1158e-01,\n",
      "         -5.5740e-01, -4.1706e-01, -4.3483e-01, -4.8113e-01,  7.0615e-01,\n",
      "          7.1174e-01,  1.3698e-01,  1.0901e+00,  4.0756e-01,  1.8022e-01,\n",
      "          1.3850e+00,  2.7536e-01,  3.9892e-01, -7.8716e-01,  3.6407e-01,\n",
      "          1.4808e-01,  1.5862e-01, -2.2350e-01, -1.5621e-01,  4.8113e-01,\n",
      "          3.3822e-01, -4.2926e-01,  1.3354e-01, -2.0927e-01,  2.7253e-02,\n",
      "         -5.7329e-01,  6.1941e-01,  6.6718e-01, -3.2725e-01,  9.7755e-02,\n",
      "          9.6987e-01,  1.6423e-01,  9.1480e-01, -1.3486e-01,  6.7320e-01,\n",
      "         -6.4783e-01, -3.3928e-01, -5.8368e-01,  6.9368e-02,  8.6673e-01,\n",
      "          4.9012e-01, -1.9347e-01,  3.3211e-01,  4.0870e-02,  4.1980e-01,\n",
      "          3.9780e-01, -2.6530e-01,  3.7234e-02]], grad_fn=<AddmmBackward>)\n",
      "layer2 tensor([[5.6990e-03, 1.4555e-02, 2.1026e-04, 1.0000e+00, 9.9194e-01, 1.2385e-03,\n",
      "         3.2999e-06, 1.0000e+00, 3.8347e-06, 9.6219e-04, 9.8856e-01, 9.9976e-01,\n",
      "         9.9999e-01, 4.3556e-02, 9.8307e-01, 1.3391e-03, 7.6510e-02, 9.9910e-01,\n",
      "         2.5612e-01, 9.8922e-01, 5.6587e-11, 9.7271e-01, 7.9179e-01, 9.8898e-01,\n",
      "         8.5560e-02, 4.0583e-01, 6.6295e-03, 9.9116e-01, 2.0304e-07, 1.9745e-02,\n",
      "         9.9306e-01, 9.9999e-01, 8.1357e-04, 1.8855e-02, 2.9339e-01, 1.0000e+00,\n",
      "         9.9805e-01, 9.9947e-01, 1.0000e+00, 9.9305e-01, 9.9881e-01, 7.0294e-04,\n",
      "         9.9998e-01, 5.3025e-05, 5.4742e-01, 5.8669e-06, 4.1643e-04, 2.2762e-03,\n",
      "         1.0000e+00, 7.6257e-04, 6.0128e-09, 9.9824e-01, 1.3919e-05, 6.1489e-01,\n",
      "         1.3088e-02, 9.9962e-01, 2.3643e-07, 9.9994e-01, 9.7901e-01, 9.0123e-01,\n",
      "         9.9833e-01, 5.4760e-09, 1.0000e+00, 7.4043e-01, 1.0000e+00, 3.7846e-04,\n",
      "         1.0000e+00, 6.9034e-12, 1.0000e+00, 1.0656e-09, 9.0344e-01, 4.9592e-04,\n",
      "         9.9950e-01, 9.3887e-01, 9.9367e-01, 6.5486e-03, 1.0000e+00, 5.0636e-04,\n",
      "         9.9962e-01, 9.9999e-01, 2.7694e-01, 6.1757e-05, 1.1406e-04, 4.3113e-05,\n",
      "         9.9977e-01, 1.0000e+00, 1.1944e-04, 1.0000e+00, 9.9998e-01, 9.9989e-01,\n",
      "         1.0000e+00, 1.0000e+00, 9.9307e-01, 1.4158e-07, 8.9988e-01, 8.9024e-01,\n",
      "         5.2285e-01, 2.2653e-03, 8.1912e-03, 8.6312e-01, 9.1895e-01, 1.4510e-01,\n",
      "         7.1280e-04, 9.7065e-01, 1.9718e-01, 1.1026e-06, 9.9999e-01, 9.9866e-01,\n",
      "         3.5065e-01, 5.5243e-01, 1.0000e+00, 9.9429e-01, 1.0000e+00, 8.2163e-06,\n",
      "         1.0000e+00, 1.4231e-06, 1.4377e-01, 9.7967e-05, 1.9092e-02, 9.3449e-01,\n",
      "         1.0000e+00, 9.9939e-01, 9.9999e-01, 9.9676e-01, 9.9422e-01, 9.9821e-01,\n",
      "         6.1137e-04, 6.0524e-01],\n",
      "        [4.8996e-01, 4.7520e-01, 4.7754e-01, 4.9598e-01, 5.3558e-01, 4.9591e-01,\n",
      "         5.1208e-01, 4.6160e-01, 4.7307e-01, 4.8742e-01, 4.7396e-01, 5.2331e-01,\n",
      "         5.0601e-01, 5.1866e-01, 5.1740e-01, 5.2016e-01, 4.8897e-01, 5.3637e-01,\n",
      "         5.3344e-01, 5.2017e-01, 4.9295e-01, 4.7499e-01, 5.3034e-01, 5.1989e-01,\n",
      "         4.9276e-01, 4.7267e-01, 4.9204e-01, 4.7570e-01, 4.5705e-01, 5.3419e-01,\n",
      "         4.8268e-01, 5.2612e-01, 5.1257e-01, 5.0676e-01, 4.9420e-01, 5.3771e-01,\n",
      "         5.0196e-01, 5.1716e-01, 5.0888e-01, 5.1530e-01, 4.7878e-01, 5.4110e-01,\n",
      "         4.6055e-01, 4.9394e-01, 5.2467e-01, 5.1804e-01, 5.2302e-01, 5.0511e-01,\n",
      "         5.0642e-01, 5.3191e-01, 4.8617e-01, 4.7378e-01, 4.9338e-01, 5.0964e-01,\n",
      "         4.7254e-01, 4.8588e-01, 4.5700e-01, 4.7927e-01, 4.8727e-01, 5.3826e-01,\n",
      "         4.6146e-01, 4.6401e-01, 5.1048e-01, 4.6282e-01, 4.8775e-01, 4.7431e-01,\n",
      "         4.9576e-01, 4.6014e-01, 5.1038e-01, 5.1811e-01, 5.3750e-01, 4.8493e-01,\n",
      "         4.8955e-01, 4.9044e-01, 4.7538e-01, 4.6325e-01, 4.6220e-01, 4.6976e-01,\n",
      "         5.2552e-01, 4.8668e-01, 5.0617e-01, 4.6528e-01, 5.2360e-01, 5.0631e-01,\n",
      "         4.7414e-01, 4.6927e-01, 5.0612e-01, 5.1834e-01, 4.6991e-01, 4.8599e-01,\n",
      "         5.1763e-01, 4.7714e-01, 4.6014e-01, 4.8135e-01, 4.8426e-01, 5.3693e-01,\n",
      "         5.0366e-01, 4.8314e-01, 4.9453e-01, 5.2225e-01, 4.9758e-01, 5.1672e-01,\n",
      "         5.1911e-01, 4.5751e-01, 5.4344e-01, 5.1723e-01, 5.0507e-01, 5.2822e-01,\n",
      "         5.1004e-01, 5.2167e-01, 4.6824e-01, 5.0743e-01, 4.9211e-01, 4.9652e-01,\n",
      "         5.0109e-01, 4.5663e-01, 4.6940e-01, 4.8101e-01, 5.4343e-01, 5.2166e-01,\n",
      "         4.9517e-01, 5.0414e-01, 4.9397e-01, 5.1528e-01, 5.3515e-01, 4.6726e-01,\n",
      "         4.8658e-01, 4.9356e-01],\n",
      "        [5.0560e-01, 4.0376e-01, 3.5008e-01, 6.2299e-01, 5.9535e-01, 4.7145e-01,\n",
      "         4.0753e-01, 6.4265e-01, 3.7414e-01, 3.1740e-01, 5.0837e-01, 5.1629e-01,\n",
      "         6.2226e-01, 4.7491e-01, 5.6178e-01, 3.6937e-01, 4.5727e-01, 6.0466e-01,\n",
      "         4.8383e-01, 4.8952e-01, 2.5563e-01, 4.7020e-01, 5.0502e-01, 6.4838e-01,\n",
      "         5.4790e-01, 5.0939e-01, 4.1990e-01, 4.6470e-01, 2.9107e-01, 4.0729e-01,\n",
      "         5.7206e-01, 5.4374e-01, 4.3837e-01, 3.9510e-01, 4.6336e-01, 7.6097e-01,\n",
      "         5.0964e-01, 5.3108e-01, 6.3383e-01, 6.9757e-01, 5.2770e-01, 4.3422e-01,\n",
      "         5.5132e-01, 3.5770e-01, 4.9033e-01, 3.4879e-01, 4.3726e-01, 4.8070e-01,\n",
      "         6.6674e-01, 5.5302e-01, 1.7078e-01, 4.7493e-01, 3.4765e-01, 5.9336e-01,\n",
      "         4.7569e-01, 5.8255e-01, 3.1630e-01, 6.8387e-01, 4.6550e-01, 4.8975e-01,\n",
      "         5.0356e-01, 2.6022e-01, 7.0157e-01, 5.7209e-01, 6.8093e-01, 3.5432e-01,\n",
      "         6.7154e-01, 1.2801e-01, 5.4478e-01, 2.5107e-01, 4.9071e-01, 3.7256e-01,\n",
      "         5.1330e-01, 4.6780e-01, 5.2112e-01, 3.4373e-01, 5.0287e-01, 5.1215e-01,\n",
      "         7.5274e-01, 7.1332e-01, 3.6415e-01, 3.9722e-01, 3.9297e-01, 3.8199e-01,\n",
      "         6.6955e-01, 6.7078e-01, 5.3419e-01, 7.4840e-01, 6.0050e-01, 5.4493e-01,\n",
      "         7.9980e-01, 5.6841e-01, 5.9843e-01, 3.1278e-01, 5.9003e-01, 5.3695e-01,\n",
      "         5.3957e-01, 4.4436e-01, 4.6103e-01, 6.1802e-01, 5.8376e-01, 3.9430e-01,\n",
      "         5.3334e-01, 4.4787e-01, 5.0681e-01, 3.6048e-01, 6.5009e-01, 6.6087e-01,\n",
      "         4.1891e-01, 5.2442e-01, 7.2509e-01, 5.4096e-01, 7.1398e-01, 4.6634e-01,\n",
      "         6.6222e-01, 3.4348e-01, 4.1598e-01, 3.5809e-01, 5.1734e-01, 7.0406e-01,\n",
      "         6.2013e-01, 4.5178e-01, 5.8227e-01, 5.1022e-01, 6.0343e-01, 5.9816e-01,\n",
      "         4.3406e-01, 5.0931e-01]], grad_fn=<SigmoidBackward>)\n",
      "layer3 tensor([[ 0.1005,  0.3687, -0.0760, -0.0875, -0.4301,  0.0443, -0.0336, -1.2420,\n",
      "          0.7053,  0.5148],\n",
      "        [-0.0223,  0.0314, -0.0253,  0.1437, -0.3144,  0.0723, -0.2724, -0.8714,\n",
      "          0.2898, -0.0378],\n",
      "        [-0.0360,  0.1826, -0.0489,  0.1334, -0.3620,  0.0288, -0.3109, -0.8929,\n",
      "          0.3289, -0.0013]], grad_fn=<AddmmBackward>)\n",
      "tensor([[ 0.0038,  0.0036,  0.0035,  ..., -0.0001, -0.0003, -0.0004],\n",
      "        [-0.0033, -0.0034, -0.0034,  ..., -0.0044, -0.0044, -0.0045],\n",
      "        [ 0.0017,  0.0016,  0.0016,  ...,  0.0015,  0.0015,  0.0014],\n",
      "        ...,\n",
      "        [ 0.0056,  0.0055,  0.0055,  ...,  0.0050,  0.0049,  0.0049],\n",
      "        [-0.0069, -0.0069, -0.0069,  ..., -0.0065, -0.0065, -0.0065],\n",
      "        [ 0.0001, -0.0022, -0.0045,  ..., -0.0678, -0.0702, -0.0725]])\n",
      "tensor([ 1.1053e-02, -7.9712e-03,  1.1419e-03, -1.1163e-03, -1.7514e-03,\n",
      "        -3.0554e-03,  5.8400e-03,  7.3465e-03, -5.8553e-03, -7.7193e-03,\n",
      "        -2.8849e-03, -6.6578e-03, -2.0644e-03,  1.0595e-02, -7.5451e-03,\n",
      "         8.6339e-04, -2.0560e-03,  6.7216e-04,  4.2553e-04, -9.6274e-04,\n",
      "        -7.1123e-05, -1.0189e-02, -4.4697e-04, -2.0282e-03,  1.2202e-02,\n",
      "        -1.1080e-03,  1.5325e-03,  4.2261e-03, -1.6875e-03, -4.1436e-03,\n",
      "        -1.1148e-06, -1.8092e-03, -1.6112e-03, -5.6564e-03, -8.3603e-03,\n",
      "         2.7277e-04,  6.2027e-03,  8.7601e-03, -4.2121e-03,  3.5945e-03,\n",
      "         6.1234e-03, -1.3460e-03,  5.3441e-03, -8.5653e-03,  7.4111e-03,\n",
      "         9.4816e-03,  2.5371e-03, -4.0386e-03,  1.6369e-03, -7.9973e-04,\n",
      "         3.6706e-03,  9.7085e-03, -5.4229e-03,  1.4975e-03,  4.7115e-03,\n",
      "         5.8028e-04, -1.0348e-03, -2.6955e-03, -4.1408e-03, -3.7796e-03,\n",
      "        -2.0458e-03, -6.1033e-03, -4.4063e-03, -2.0304e-03, -3.9989e-03,\n",
      "        -4.5839e-03, -5.4344e-03, -2.8434e-04, -7.0599e-04, -4.1520e-04,\n",
      "         3.0908e-03,  5.1639e-03,  1.0426e-02, -3.9041e-03,  3.5714e-03,\n",
      "        -1.1175e-03,  3.3210e-03, -4.1493e-03, -3.9681e-03, -2.6664e-03,\n",
      "        -4.6582e-04,  4.2595e-03, -8.7967e-03,  7.5984e-03, -5.8358e-03,\n",
      "        -1.0227e-03,  2.2322e-03,  3.8136e-03,  5.0387e-03,  5.3472e-04,\n",
      "         3.7599e-03,  2.1031e-03,  6.6784e-03,  4.3863e-03,  2.2561e-03,\n",
      "        -6.9499e-03,  2.2206e-03,  9.5021e-03,  9.4306e-03, -6.2350e-03,\n",
      "         3.8523e-03, -7.9925e-03, -9.0832e-03,  4.0541e-04, -2.2858e-03,\n",
      "        -2.1944e-03,  3.2116e-03, -1.8960e-03,  8.1368e-03, -5.2954e-03,\n",
      "        -4.5954e-03, -1.1920e-03,  2.1201e-03, -8.0896e-03,  3.6477e-03,\n",
      "        -4.8088e-03,  5.6593e-03,  5.8307e-03,  4.5926e-03, -6.9454e-03,\n",
      "        -3.1711e-03,  1.3889e-03, -4.2339e-03,  2.5762e-03, -2.9295e-03,\n",
      "         1.6089e-03, -2.2102e-04, -5.5866e-03])\n",
      "tensor([[ 0.0323,  0.0257,  0.0283,  ..., -0.2630,  0.0313, -0.1473],\n",
      "        [ 0.0394,  0.0350,  0.0322,  ...,  0.0858,  0.0360,  0.0658],\n",
      "        [ 0.0339,  0.0303,  0.0282,  ...,  0.0640,  0.0313,  0.0509],\n",
      "        ...,\n",
      "        [ 0.0146,  0.0130,  0.0121,  ...,  0.0242,  0.0134,  0.0199],\n",
      "        [ 0.0481,  0.0430,  0.0396,  ...,  0.1122,  0.0442,  0.0851],\n",
      "        [-0.1339, -0.1035, -0.0882,  ..., -0.1121, -0.1129, -0.1046]])\n",
      "tensor([-0.2316,  0.1222,  0.0958, -0.2248,  0.0699,  0.1056,  0.0818,  0.0379,\n",
      "         0.1570, -0.2138])\n",
      "############################\n",
      "layer1 tensor([[-5.1618e+00, -4.2152e+00, -8.4670e+00,  1.4110e+01,  4.8130e+00,\n",
      "         -6.6926e+00, -1.2622e+01,  1.4639e+01, -1.2471e+01, -6.9453e+00,\n",
      "          4.4591e+00,  8.3486e+00,  1.1296e+01, -3.0892e+00,  4.0617e+00,\n",
      "         -6.6144e+00, -2.4907e+00,  7.0176e+00, -1.0662e+00,  4.5190e+00,\n",
      "         -2.3595e+01,  3.5734e+00,  1.3358e+00,  4.4969e+00, -2.3691e+00,\n",
      "         -3.8123e-01, -5.0096e+00,  4.7195e+00, -1.5410e+01, -3.9049e+00,\n",
      "          4.9635e+00,  1.1775e+01, -7.1133e+00, -3.9520e+00, -8.7899e-01,\n",
      "          1.7951e+01,  6.2363e+00,  7.5335e+00,  1.3576e+01,  4.9625e+00,\n",
      "          6.7328e+00, -7.2595e+00,  1.0612e+01, -9.8447e+00,  1.9025e-01,\n",
      "         -1.2046e+01, -7.7834e+00, -6.0830e+00,  1.2308e+01, -7.1781e+00,\n",
      "         -1.8929e+01,  6.3393e+00, -1.1182e+01,  4.6790e-01, -4.3229e+00,\n",
      "          7.8729e+00, -1.5258e+01,  9.7835e+00,  3.8424e+00,  2.2110e+00,\n",
      "          6.3909e+00, -1.9023e+01,  1.4249e+01,  1.0482e+00,  1.2440e+01,\n",
      "         -7.8790e+00,  1.6814e+01, -2.5699e+01,  1.2589e+01, -2.0660e+01,\n",
      "          2.2361e+00, -7.6086e+00,  7.5920e+00,  2.7316e+00,  5.0562e+00,\n",
      "         -5.0219e+00,  1.7953e+01, -7.5878e+00,  7.8789e+00,  1.1470e+01,\n",
      "         -9.5968e-01, -9.6922e+00, -9.0786e+00, -1.0052e+01,  8.3738e+00,\n",
      "          1.7142e+01, -9.0326e+00,  1.3228e+01,  1.0658e+01,  9.1353e+00,\n",
      "          1.6991e+01,  1.3973e+01,  4.9652e+00, -1.5770e+01,  2.1958e+00,\n",
      "          2.0932e+00,  9.1460e-02, -6.0878e+00, -4.7965e+00,  1.8414e+00,\n",
      "          2.4282e+00, -1.7735e+00, -7.2456e+00,  3.4987e+00, -1.4040e+00,\n",
      "         -1.3718e+01,  1.2135e+01,  6.6168e+00, -6.1617e-01,  2.1049e-01,\n",
      "          2.0342e+01,  5.1593e+00,  2.2236e+01, -1.1709e+01,  1.3859e+01,\n",
      "         -1.3463e+01, -1.7843e+00, -9.2308e+00, -3.9392e+00,  2.6577e+00,\n",
      "          1.2801e+01,  7.4037e+00,  1.1784e+01,  5.7293e+00,  5.1478e+00,\n",
      "          6.3228e+00, -7.3992e+00,  4.2733e-01],\n",
      "        [-4.0159e-02, -9.9294e-02, -8.9895e-02, -1.6091e-02,  1.4256e-01,\n",
      "         -1.6360e-02,  4.8316e-02, -1.5389e-01, -1.0784e-01, -5.0343e-02,\n",
      "         -1.0425e-01,  9.3292e-02,  2.4054e-02,  7.4690e-02,  6.9636e-02,\n",
      "          8.0688e-02, -4.4123e-02,  1.4575e-01,  1.3394e-01,  8.0742e-02,\n",
      "         -2.8184e-02, -1.0012e-01,  1.2153e-01,  7.9604e-02, -2.8956e-02,\n",
      "         -1.0943e-01, -3.1854e-02, -9.7265e-02, -1.7223e-01,  1.3697e-01,\n",
      "         -6.9313e-02,  1.0456e-01,  5.0276e-02,  2.7027e-02, -2.3206e-02,\n",
      "          1.5111e-01,  7.8399e-03,  6.8676e-02,  3.5505e-02,  6.1230e-02,\n",
      "         -8.4925e-02,  1.6478e-01, -1.5812e-01, -2.4261e-02,  9.8777e-02,\n",
      "          7.2197e-02,  9.2127e-02,  2.0429e-02,  2.5663e-02,  1.2782e-01,\n",
      "         -5.5344e-02, -1.0498e-01, -2.6496e-02,  3.8555e-02, -1.0996e-01,\n",
      "         -5.6475e-02, -1.7242e-01, -8.2958e-02, -5.0914e-02,  1.5333e-01,\n",
      "         -1.5445e-01, -1.4422e-01,  4.1916e-02, -1.4901e-01, -4.8993e-02,\n",
      "         -1.0284e-01, -1.6944e-02, -1.5977e-01,  4.1540e-02,  7.2491e-02,\n",
      "          1.5028e-01, -6.0289e-02, -4.1806e-02, -3.8239e-02, -9.8579e-02,\n",
      "         -1.4725e-01, -1.5148e-01, -1.2111e-01,  1.0216e-01, -5.3280e-02,\n",
      "          2.4666e-02, -1.3911e-01,  9.4461e-02,  2.5234e-02, -1.0352e-01,\n",
      "         -1.2309e-01,  2.4462e-02,  7.3409e-02, -1.2050e-01, -5.6063e-02,\n",
      "          7.0540e-02, -9.1485e-02, -1.5976e-01, -7.4644e-02, -6.2973e-02,\n",
      "          1.4798e-01,  1.4651e-02, -6.7457e-02, -2.1890e-02,  8.9072e-02,\n",
      "         -9.6915e-03,  6.6886e-02,  7.6476e-02, -1.7036e-01,  1.7421e-01,\n",
      "          6.8963e-02,  2.0286e-02,  1.1301e-01,  4.0170e-02,  8.6716e-02,\n",
      "         -1.2721e-01,  2.9738e-02, -3.1544e-02, -1.3908e-02,  4.3627e-03,\n",
      "         -1.7392e-01, -1.2254e-01, -7.6004e-02,  1.7416e-01,  8.6714e-02,\n",
      "         -1.9331e-02,  1.6562e-02, -2.4129e-02,  6.1151e-02,  1.4085e-01,\n",
      "         -1.3116e-01, -5.3703e-02, -2.5768e-02],\n",
      "        [ 2.2405e-02, -3.8982e-01, -6.1869e-01,  5.0226e-01,  3.8613e-01,\n",
      "         -1.1434e-01, -3.7417e-01,  5.8687e-01, -5.1449e-01, -7.6573e-01,\n",
      "          3.3475e-02,  6.5186e-02,  4.9915e-01, -1.0044e-01,  2.4841e-01,\n",
      "         -5.3491e-01, -1.7133e-01,  4.2491e-01, -6.4686e-02, -4.1935e-02,\n",
      "         -1.0688e+00, -1.1936e-01,  2.0071e-02,  6.1194e-01,  1.9218e-01,\n",
      "          3.7566e-02, -3.2318e-01, -1.4143e-01, -8.9021e-01, -3.7517e-01,\n",
      "          2.9025e-01,  1.7541e-01, -2.4778e-01, -4.2591e-01, -1.4682e-01,\n",
      "          1.1580e+00,  3.8565e-02,  1.2448e-01,  5.4870e-01,  8.3577e-01,\n",
      "          1.1089e-01, -2.6464e-01,  2.0600e-01, -5.8538e-01, -3.8686e-02,\n",
      "         -6.2438e-01, -2.5229e-01, -7.7221e-02,  6.9347e-01,  2.1288e-01,\n",
      "         -1.5801e+00, -1.0036e-01, -6.2938e-01,  3.7787e-01, -9.7309e-02,\n",
      "          3.3324e-01, -7.7082e-01,  7.7159e-01, -1.3824e-01, -4.1009e-02,\n",
      "          1.4255e-02, -1.0448e+00,  8.5480e-01,  2.9040e-01,  7.5803e-01,\n",
      "         -6.0011e-01,  7.1517e-01, -1.9186e+00,  1.7959e-01, -1.0929e+00,\n",
      "         -3.7153e-02, -5.2127e-01,  5.3206e-02, -1.2899e-01,  8.4548e-02,\n",
      "         -6.4670e-01,  1.1487e-02,  4.8612e-02,  1.1133e+00,  9.1158e-01,\n",
      "         -5.5740e-01, -4.1706e-01, -4.3483e-01, -4.8113e-01,  7.0615e-01,\n",
      "          7.1174e-01,  1.3698e-01,  1.0901e+00,  4.0756e-01,  1.8022e-01,\n",
      "          1.3850e+00,  2.7536e-01,  3.9892e-01, -7.8716e-01,  3.6407e-01,\n",
      "          1.4808e-01,  1.5862e-01, -2.2350e-01, -1.5621e-01,  4.8113e-01,\n",
      "          3.3822e-01, -4.2926e-01,  1.3354e-01, -2.0927e-01,  2.7253e-02,\n",
      "         -5.7329e-01,  6.1941e-01,  6.6718e-01, -3.2725e-01,  9.7755e-02,\n",
      "          9.6987e-01,  1.6423e-01,  9.1480e-01, -1.3486e-01,  6.7320e-01,\n",
      "         -6.4783e-01, -3.3928e-01, -5.8368e-01,  6.9368e-02,  8.6673e-01,\n",
      "          4.9012e-01, -1.9347e-01,  3.3211e-01,  4.0870e-02,  4.1980e-01,\n",
      "          3.9780e-01, -2.6530e-01,  3.7234e-02]], grad_fn=<AddmmBackward>)\n",
      "layer2 tensor([[5.6990e-03, 1.4555e-02, 2.1026e-04, 1.0000e+00, 9.9194e-01, 1.2385e-03,\n",
      "         3.2999e-06, 1.0000e+00, 3.8347e-06, 9.6219e-04, 9.8856e-01, 9.9976e-01,\n",
      "         9.9999e-01, 4.3556e-02, 9.8307e-01, 1.3391e-03, 7.6510e-02, 9.9910e-01,\n",
      "         2.5612e-01, 9.8922e-01, 5.6587e-11, 9.7271e-01, 7.9179e-01, 9.8898e-01,\n",
      "         8.5560e-02, 4.0583e-01, 6.6295e-03, 9.9116e-01, 2.0304e-07, 1.9745e-02,\n",
      "         9.9306e-01, 9.9999e-01, 8.1357e-04, 1.8855e-02, 2.9339e-01, 1.0000e+00,\n",
      "         9.9805e-01, 9.9947e-01, 1.0000e+00, 9.9305e-01, 9.9881e-01, 7.0294e-04,\n",
      "         9.9998e-01, 5.3025e-05, 5.4742e-01, 5.8669e-06, 4.1643e-04, 2.2762e-03,\n",
      "         1.0000e+00, 7.6257e-04, 6.0128e-09, 9.9824e-01, 1.3919e-05, 6.1489e-01,\n",
      "         1.3088e-02, 9.9962e-01, 2.3643e-07, 9.9994e-01, 9.7901e-01, 9.0123e-01,\n",
      "         9.9833e-01, 5.4760e-09, 1.0000e+00, 7.4043e-01, 1.0000e+00, 3.7846e-04,\n",
      "         1.0000e+00, 6.9034e-12, 1.0000e+00, 1.0656e-09, 9.0344e-01, 4.9592e-04,\n",
      "         9.9950e-01, 9.3887e-01, 9.9367e-01, 6.5486e-03, 1.0000e+00, 5.0636e-04,\n",
      "         9.9962e-01, 9.9999e-01, 2.7694e-01, 6.1757e-05, 1.1406e-04, 4.3113e-05,\n",
      "         9.9977e-01, 1.0000e+00, 1.1944e-04, 1.0000e+00, 9.9998e-01, 9.9989e-01,\n",
      "         1.0000e+00, 1.0000e+00, 9.9307e-01, 1.4158e-07, 8.9988e-01, 8.9024e-01,\n",
      "         5.2285e-01, 2.2653e-03, 8.1912e-03, 8.6312e-01, 9.1895e-01, 1.4510e-01,\n",
      "         7.1280e-04, 9.7065e-01, 1.9718e-01, 1.1026e-06, 9.9999e-01, 9.9866e-01,\n",
      "         3.5065e-01, 5.5243e-01, 1.0000e+00, 9.9429e-01, 1.0000e+00, 8.2163e-06,\n",
      "         1.0000e+00, 1.4231e-06, 1.4377e-01, 9.7967e-05, 1.9092e-02, 9.3449e-01,\n",
      "         1.0000e+00, 9.9939e-01, 9.9999e-01, 9.9676e-01, 9.9422e-01, 9.9821e-01,\n",
      "         6.1137e-04, 6.0524e-01],\n",
      "        [4.8996e-01, 4.7520e-01, 4.7754e-01, 4.9598e-01, 5.3558e-01, 4.9591e-01,\n",
      "         5.1208e-01, 4.6160e-01, 4.7307e-01, 4.8742e-01, 4.7396e-01, 5.2331e-01,\n",
      "         5.0601e-01, 5.1866e-01, 5.1740e-01, 5.2016e-01, 4.8897e-01, 5.3637e-01,\n",
      "         5.3344e-01, 5.2017e-01, 4.9295e-01, 4.7499e-01, 5.3034e-01, 5.1989e-01,\n",
      "         4.9276e-01, 4.7267e-01, 4.9204e-01, 4.7570e-01, 4.5705e-01, 5.3419e-01,\n",
      "         4.8268e-01, 5.2612e-01, 5.1257e-01, 5.0676e-01, 4.9420e-01, 5.3771e-01,\n",
      "         5.0196e-01, 5.1716e-01, 5.0888e-01, 5.1530e-01, 4.7878e-01, 5.4110e-01,\n",
      "         4.6055e-01, 4.9394e-01, 5.2467e-01, 5.1804e-01, 5.2302e-01, 5.0511e-01,\n",
      "         5.0642e-01, 5.3191e-01, 4.8617e-01, 4.7378e-01, 4.9338e-01, 5.0964e-01,\n",
      "         4.7254e-01, 4.8588e-01, 4.5700e-01, 4.7927e-01, 4.8727e-01, 5.3826e-01,\n",
      "         4.6146e-01, 4.6401e-01, 5.1048e-01, 4.6282e-01, 4.8775e-01, 4.7431e-01,\n",
      "         4.9576e-01, 4.6014e-01, 5.1038e-01, 5.1811e-01, 5.3750e-01, 4.8493e-01,\n",
      "         4.8955e-01, 4.9044e-01, 4.7538e-01, 4.6325e-01, 4.6220e-01, 4.6976e-01,\n",
      "         5.2552e-01, 4.8668e-01, 5.0617e-01, 4.6528e-01, 5.2360e-01, 5.0631e-01,\n",
      "         4.7414e-01, 4.6927e-01, 5.0612e-01, 5.1834e-01, 4.6991e-01, 4.8599e-01,\n",
      "         5.1763e-01, 4.7714e-01, 4.6014e-01, 4.8135e-01, 4.8426e-01, 5.3693e-01,\n",
      "         5.0366e-01, 4.8314e-01, 4.9453e-01, 5.2225e-01, 4.9758e-01, 5.1672e-01,\n",
      "         5.1911e-01, 4.5751e-01, 5.4344e-01, 5.1723e-01, 5.0507e-01, 5.2822e-01,\n",
      "         5.1004e-01, 5.2167e-01, 4.6824e-01, 5.0743e-01, 4.9211e-01, 4.9652e-01,\n",
      "         5.0109e-01, 4.5663e-01, 4.6940e-01, 4.8101e-01, 5.4343e-01, 5.2166e-01,\n",
      "         4.9517e-01, 5.0414e-01, 4.9397e-01, 5.1528e-01, 5.3515e-01, 4.6726e-01,\n",
      "         4.8658e-01, 4.9356e-01],\n",
      "        [5.0560e-01, 4.0376e-01, 3.5008e-01, 6.2299e-01, 5.9535e-01, 4.7145e-01,\n",
      "         4.0753e-01, 6.4265e-01, 3.7414e-01, 3.1740e-01, 5.0837e-01, 5.1629e-01,\n",
      "         6.2226e-01, 4.7491e-01, 5.6178e-01, 3.6937e-01, 4.5727e-01, 6.0466e-01,\n",
      "         4.8383e-01, 4.8952e-01, 2.5563e-01, 4.7020e-01, 5.0502e-01, 6.4838e-01,\n",
      "         5.4790e-01, 5.0939e-01, 4.1990e-01, 4.6470e-01, 2.9107e-01, 4.0729e-01,\n",
      "         5.7206e-01, 5.4374e-01, 4.3837e-01, 3.9510e-01, 4.6336e-01, 7.6097e-01,\n",
      "         5.0964e-01, 5.3108e-01, 6.3383e-01, 6.9757e-01, 5.2770e-01, 4.3422e-01,\n",
      "         5.5132e-01, 3.5770e-01, 4.9033e-01, 3.4879e-01, 4.3726e-01, 4.8070e-01,\n",
      "         6.6674e-01, 5.5302e-01, 1.7078e-01, 4.7493e-01, 3.4765e-01, 5.9336e-01,\n",
      "         4.7569e-01, 5.8255e-01, 3.1630e-01, 6.8387e-01, 4.6550e-01, 4.8975e-01,\n",
      "         5.0356e-01, 2.6022e-01, 7.0157e-01, 5.7209e-01, 6.8093e-01, 3.5432e-01,\n",
      "         6.7154e-01, 1.2801e-01, 5.4478e-01, 2.5107e-01, 4.9071e-01, 3.7256e-01,\n",
      "         5.1330e-01, 4.6780e-01, 5.2112e-01, 3.4373e-01, 5.0287e-01, 5.1215e-01,\n",
      "         7.5274e-01, 7.1332e-01, 3.6415e-01, 3.9722e-01, 3.9297e-01, 3.8199e-01,\n",
      "         6.6955e-01, 6.7078e-01, 5.3419e-01, 7.4840e-01, 6.0050e-01, 5.4493e-01,\n",
      "         7.9980e-01, 5.6841e-01, 5.9843e-01, 3.1278e-01, 5.9003e-01, 5.3695e-01,\n",
      "         5.3957e-01, 4.4436e-01, 4.6103e-01, 6.1802e-01, 5.8376e-01, 3.9430e-01,\n",
      "         5.3334e-01, 4.4787e-01, 5.0681e-01, 3.6048e-01, 6.5009e-01, 6.6087e-01,\n",
      "         4.1891e-01, 5.2442e-01, 7.2509e-01, 5.4096e-01, 7.1398e-01, 4.6634e-01,\n",
      "         6.6222e-01, 3.4348e-01, 4.1598e-01, 3.5809e-01, 5.1734e-01, 7.0406e-01,\n",
      "         6.2013e-01, 4.5178e-01, 5.8227e-01, 5.1022e-01, 6.0343e-01, 5.9816e-01,\n",
      "         4.3406e-01, 5.0931e-01]], grad_fn=<SigmoidBackward>)\n",
      "layer3 tensor([[ 0.1005,  0.3687, -0.0760, -0.0875, -0.4301,  0.0443, -0.0336, -1.2420,\n",
      "          0.7053,  0.5148],\n",
      "        [-0.0223,  0.0314, -0.0253,  0.1437, -0.3144,  0.0723, -0.2724, -0.8714,\n",
      "          0.2898, -0.0378],\n",
      "        [-0.0360,  0.1826, -0.0489,  0.1334, -0.3620,  0.0288, -0.3109, -0.8929,\n",
      "          0.3289, -0.0013]], grad_fn=<AddmmBackward>)\n",
      "tensor([[ 0.0038,  0.0036,  0.0035,  ..., -0.0001, -0.0003, -0.0004],\n",
      "        [-0.0033, -0.0034, -0.0034,  ..., -0.0044, -0.0044, -0.0045],\n",
      "        [ 0.0017,  0.0016,  0.0016,  ...,  0.0015,  0.0015,  0.0014],\n",
      "        ...,\n",
      "        [ 0.0056,  0.0055,  0.0055,  ...,  0.0050,  0.0049,  0.0049],\n",
      "        [-0.0069, -0.0069, -0.0069,  ..., -0.0065, -0.0065, -0.0065],\n",
      "        [ 0.0001, -0.0022, -0.0045,  ..., -0.0678, -0.0702, -0.0725]])\n",
      "tensor([ 1.1053e-02, -7.9712e-03,  1.1419e-03, -1.1163e-03, -1.7514e-03,\n",
      "        -3.0554e-03,  5.8400e-03,  7.3465e-03, -5.8553e-03, -7.7193e-03,\n",
      "        -2.8849e-03, -6.6578e-03, -2.0644e-03,  1.0595e-02, -7.5451e-03,\n",
      "         8.6339e-04, -2.0560e-03,  6.7216e-04,  4.2553e-04, -9.6274e-04,\n",
      "        -7.1123e-05, -1.0189e-02, -4.4697e-04, -2.0282e-03,  1.2202e-02,\n",
      "        -1.1080e-03,  1.5325e-03,  4.2261e-03, -1.6875e-03, -4.1436e-03,\n",
      "        -1.1148e-06, -1.8092e-03, -1.6112e-03, -5.6564e-03, -8.3603e-03,\n",
      "         2.7277e-04,  6.2027e-03,  8.7601e-03, -4.2121e-03,  3.5945e-03,\n",
      "         6.1234e-03, -1.3460e-03,  5.3441e-03, -8.5653e-03,  7.4111e-03,\n",
      "         9.4816e-03,  2.5371e-03, -4.0386e-03,  1.6369e-03, -7.9973e-04,\n",
      "         3.6706e-03,  9.7085e-03, -5.4229e-03,  1.4975e-03,  4.7115e-03,\n",
      "         5.8028e-04, -1.0348e-03, -2.6955e-03, -4.1408e-03, -3.7796e-03,\n",
      "        -2.0458e-03, -6.1033e-03, -4.4063e-03, -2.0304e-03, -3.9989e-03,\n",
      "        -4.5839e-03, -5.4344e-03, -2.8434e-04, -7.0599e-04, -4.1520e-04,\n",
      "         3.0908e-03,  5.1639e-03,  1.0426e-02, -3.9041e-03,  3.5714e-03,\n",
      "        -1.1175e-03,  3.3210e-03, -4.1493e-03, -3.9681e-03, -2.6664e-03,\n",
      "        -4.6582e-04,  4.2595e-03, -8.7967e-03,  7.5984e-03, -5.8358e-03,\n",
      "        -1.0227e-03,  2.2322e-03,  3.8136e-03,  5.0387e-03,  5.3472e-04,\n",
      "         3.7599e-03,  2.1031e-03,  6.6784e-03,  4.3863e-03,  2.2561e-03,\n",
      "        -6.9499e-03,  2.2206e-03,  9.5021e-03,  9.4306e-03, -6.2350e-03,\n",
      "         3.8523e-03, -7.9925e-03, -9.0832e-03,  4.0541e-04, -2.2858e-03,\n",
      "        -2.1944e-03,  3.2116e-03, -1.8960e-03,  8.1368e-03, -5.2954e-03,\n",
      "        -4.5954e-03, -1.1920e-03,  2.1201e-03, -8.0896e-03,  3.6477e-03,\n",
      "        -4.8088e-03,  5.6593e-03,  5.8307e-03,  4.5926e-03, -6.9454e-03,\n",
      "        -3.1711e-03,  1.3889e-03, -4.2339e-03,  2.5762e-03, -2.9295e-03,\n",
      "         1.6089e-03, -2.2102e-04, -5.5866e-03])\n",
      "tensor([[ 0.0323,  0.0257,  0.0283,  ..., -0.2630,  0.0313, -0.1473],\n",
      "        [ 0.0394,  0.0350,  0.0322,  ...,  0.0858,  0.0360,  0.0658],\n",
      "        [ 0.0339,  0.0303,  0.0282,  ...,  0.0640,  0.0313,  0.0509],\n",
      "        ...,\n",
      "        [ 0.0146,  0.0130,  0.0121,  ...,  0.0242,  0.0134,  0.0199],\n",
      "        [ 0.0481,  0.0430,  0.0396,  ...,  0.1122,  0.0442,  0.0851],\n",
      "        [-0.1339, -0.1035, -0.0882,  ..., -0.1121, -0.1129, -0.1046]])\n",
      "tensor([-0.2316,  0.1222,  0.0958, -0.2248,  0.0699,  0.1056,  0.0818,  0.0379,\n",
      "         0.1570, -0.2138])\n"
     ]
    }
   ],
   "source": [
    "# if I forward prop and backward prop again, gradients accmulate :\n",
    "output = net(x)\n",
    "loss = criterion(output, y)\n",
    "loss.backward()\n",
    "for param in net.parameters():\n",
    "    print(param.grad)\n",
    "print(\"############################\")\n",
    "# you can remove this behavior by reinitializing the gradients in your network parameters :\n",
    "net.zero_grad()\n",
    "output = net(x)\n",
    "loss = criterion(output, y)\n",
    "loss.backward()\n",
    "for param in net.parameters():\n",
    "    print(param.grad)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# start p.11 gradient descent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parameters before gradient descent :\n",
      "Parameter containing:\n",
      "tensor([[-8.1703e-05,  8.2688e-02,  6.4926e-02,  ..., -7.4851e-02,\n",
      "         -2.7852e-02, -5.8414e-02],\n",
      "        [-1.7089e-01,  1.6789e-02, -1.0587e-02,  ..., -1.1577e-01,\n",
      "          1.6688e-01, -1.0969e-01],\n",
      "        [-6.7976e-03, -2.0125e-02, -1.0821e-01,  ...,  1.1288e-01,\n",
      "          9.5346e-02, -8.3861e-02],\n",
      "        ...,\n",
      "        [-1.5774e-01, -8.1721e-02, -4.1515e-02,  ...,  1.2808e-01,\n",
      "          9.6737e-02,  9.0107e-02],\n",
      "        [-1.0061e-01,  3.3354e-02,  2.7259e-02,  ..., -6.6138e-02,\n",
      "          6.8566e-02,  1.5990e-01],\n",
      "        [ 1.6996e-03, -2.1842e-02, -4.8077e-02,  ..., -1.7191e-01,\n",
      "         -1.7266e-01, -9.6207e-02]], requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([-0.0221, -0.0085, -0.0262, -0.1374, -0.0975,  0.1403, -0.0947,  0.1231,\n",
      "         0.0443, -0.0099, -0.0896,  0.1179, -0.0602, -0.0484,  0.1250, -0.0122,\n",
      "        -0.1419,  0.0642,  0.0364, -0.0696,  0.1424, -0.1057, -0.0374, -0.1748,\n",
      "         0.1676,  0.0943,  0.1106,  0.1072, -0.0300,  0.0849, -0.1631,  0.0047,\n",
      "         0.1577,  0.0312, -0.1616,  0.1659,  0.1710,  0.1298, -0.0934, -0.1667,\n",
      "         0.0224, -0.1603, -0.1688,  0.0320, -0.0353,  0.0202,  0.0369, -0.1479,\n",
      "         0.1465,  0.1315, -0.0943, -0.1613, -0.0874, -0.0213, -0.1461, -0.1627,\n",
      "         0.0799, -0.0607, -0.0967, -0.0537,  0.1319,  0.0368,  0.0373,  0.1063,\n",
      "        -0.1524,  0.1651,  0.0669,  0.0193,  0.0875, -0.1048, -0.1574,  0.0475,\n",
      "        -0.1186,  0.1456, -0.0405,  0.0150, -0.0618, -0.0206,  0.1448,  0.0185,\n",
      "         0.1296,  0.0490,  0.0775, -0.0330, -0.0208, -0.0788,  0.0741,  0.0818,\n",
      "        -0.1329,  0.1298, -0.0232, -0.0844,  0.0964, -0.0832, -0.0419, -0.1473,\n",
      "         0.0287, -0.1028,  0.0303,  0.1341,  0.0286,  0.1379, -0.0732, -0.1172,\n",
      "         0.0634, -0.0217, -0.0591, -0.0791,  0.1274, -0.1202,  0.0392, -0.0853,\n",
      "        -0.0287,  0.0319, -0.1748,  0.1057,  0.1457,  0.1148,  0.1281, -0.1430,\n",
      "        -0.0712,  0.0271, -0.1293,  0.0465, -0.1529,  0.0188,  0.1636, -0.1056],\n",
      "       requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([[-0.0657, -0.0471,  0.0303,  ..., -0.0253, -0.0006, -0.0398],\n",
      "        [ 0.0730, -0.0173, -0.0225,  ..., -0.0471,  0.0006,  0.0848],\n",
      "        [ 0.0402, -0.0297,  0.0142,  ..., -0.0722, -0.0686,  0.0783],\n",
      "        ...,\n",
      "        [-0.0769, -0.0702,  0.0820,  ...,  0.0179, -0.0791, -0.0544],\n",
      "        [-0.0034,  0.0673, -0.0382,  ..., -0.0479,  0.0457, -0.0156],\n",
      "        [-0.0110,  0.0694, -0.0050,  ..., -0.0179, -0.0040, -0.0756]],\n",
      "       requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([ 0.0187, -0.0796,  0.0846,  0.0692,  0.0151, -0.0514,  0.0221,  0.0448,\n",
      "        -0.0492, -0.0265], requires_grad=True)\n",
      "\n",
      "\n",
      "Parameters after gradient descetn :\n",
      "Parameter containing:\n",
      "tensor([[-0.0139,  0.0142, -0.0581,  ..., -1.6725, -1.6801, -1.7653],\n",
      "        [-0.1175,  0.0700,  0.0424,  ..., -0.0675,  0.2150, -0.0617],\n",
      "        [-0.0253, -0.0382, -0.1259,  ...,  0.1056,  0.0884, -0.0904],\n",
      "        ...,\n",
      "        [-0.1607, -0.0849, -0.0449,  ...,  0.1201,  0.0886,  0.0818],\n",
      "        [-0.0989,  0.0351,  0.0290,  ..., -0.0640,  0.0707,  0.1620],\n",
      "        [-0.0476, -0.0712, -0.0974,  ..., -0.2212, -0.2220, -0.1455]],\n",
      "       requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([-0.0457,  0.0948, -0.0079, -0.1356, -0.1544,  0.0107, -0.0222,  0.1572,\n",
      "         0.0325, -0.0655, -0.1292,  0.1821,  0.0127, -0.0412,  0.0955,  0.0003,\n",
      "        -0.1646,  0.1252, -0.0224, -0.0424,  0.1900, -0.1371,  0.0015, -0.2690,\n",
      "         0.1258,  0.0699,  0.1632,  0.0794,  0.0100,  0.2052, -0.0877,  0.0154,\n",
      "         0.1495, -0.0821, -0.2153,  0.1347,  0.2361,  0.0967, -0.0758, -0.1518,\n",
      "         0.0166, -0.1590, -0.2215,  0.0678, -0.0775, -0.0340,  0.0819, -0.2096,\n",
      "         0.2255,  0.1053, -0.0214, -0.1561, -0.0146,  0.0152, -0.0940, -0.1857,\n",
      "         0.1267, -0.0519, -0.2109, -0.1665,  0.1254,  0.1231,  0.0679, -0.0155,\n",
      "        -0.1853,  0.1078,  0.1130,  0.0349,  0.0061, -0.0697, -0.1884,  0.0389,\n",
      "        -0.1490,  0.1271, -0.0617, -0.0338, -0.0404,  0.0083,  0.1414, -0.0550,\n",
      "         0.1616,  0.0156,  0.0097, -0.0330,  0.1029, -0.0850,  0.0492,  0.0401,\n",
      "        -0.0914,  0.1472, -0.0639, -0.0414,  0.0878, -0.1337, -0.0034, -0.2307,\n",
      "        -0.0037, -0.0808,  0.0293,  0.1639, -0.1107,  0.0409, -0.1650, -0.1146,\n",
      "         0.0775, -0.0899, -0.0325, -0.0260,  0.1595, -0.0694,  0.0485, -0.1279,\n",
      "        -0.0222, -0.0183, -0.1644,  0.1953,  0.0704,  0.0894,  0.1336, -0.1122,\n",
      "        -0.0630,  0.0466, -0.1569,  0.0566, -0.1724,  0.0652,  0.1957, -0.1724],\n",
      "       requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([[ 0.7532,  2.7564,  2.7877,  ...,  2.7873,  2.7787, -0.2185],\n",
      "        [-0.1626, -0.3591, -0.3627,  ..., -0.3773, -0.3582, -0.0433],\n",
      "        [-0.6633, -1.0290, -0.9807,  ..., -1.0362, -1.1197, -0.3120],\n",
      "        ...,\n",
      "        [-0.4202, -0.6042, -0.4487,  ..., -0.5002, -0.6360, -0.2265],\n",
      "        [-0.5409, -0.7059, -0.8077,  ..., -0.7947, -0.7665, -0.3107],\n",
      "        [ 1.3044,  1.2484,  1.2035,  ...,  0.9504,  1.3201,  0.5284]],\n",
      "       requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([ 2.5912, -0.5908, -1.4309,  2.2248, -0.8610, -0.7777, -1.5297, -0.7174,\n",
      "        -1.2121,  2.3517], requires_grad=True)\n"
     ]
    }
   ],
   "source": [
    "optimizer = torch.optim.SGD(net.parameters(), lr=0.01)\n",
    "\n",
    "print(\"Parameters before gradient descent :\")\n",
    "for param in net.parameters():\n",
    "    print(param)\n",
    "optimizer.step()\n",
    "\n",
    "print(\"\\n\\nParameters after gradient descetn :\")\n",
    "for param in net.parameters():\n",
    "    print(param)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(13.5678, grad_fn=<NllLossBackward>)\n",
      "tensor(13.0338, grad_fn=<NllLossBackward>)\n",
      "tensor(12.5004, grad_fn=<NllLossBackward>)\n",
      "tensor(11.9675, grad_fn=<NllLossBackward>)\n",
      "tensor(11.4354, grad_fn=<NllLossBackward>)\n",
      "tensor(10.9041, grad_fn=<NllLossBackward>)\n",
      "tensor(10.3737, grad_fn=<NllLossBackward>)\n",
      "tensor(9.8444, grad_fn=<NllLossBackward>)\n",
      "tensor(9.3162, grad_fn=<NllLossBackward>)\n",
      "tensor(8.7893, grad_fn=<NllLossBackward>)\n",
      "tensor(8.2638, grad_fn=<NllLossBackward>)\n",
      "tensor(7.7399, grad_fn=<NllLossBackward>)\n",
      "tensor(7.2177, grad_fn=<NllLossBackward>)\n",
      "tensor(6.6975, grad_fn=<NllLossBackward>)\n",
      "tensor(6.1796, grad_fn=<NllLossBackward>)\n",
      "tensor(5.6645, grad_fn=<NllLossBackward>)\n",
      "tensor(5.1531, grad_fn=<NllLossBackward>)\n",
      "tensor(4.6469, grad_fn=<NllLossBackward>)\n",
      "tensor(4.1499, grad_fn=<NllLossBackward>)\n",
      "tensor(3.6714, grad_fn=<NllLossBackward>)\n",
      "tensor(3.2332, grad_fn=<NllLossBackward>)\n",
      "tensor(2.8752, grad_fn=<NllLossBackward>)\n",
      "tensor(2.6218, grad_fn=<NllLossBackward>)\n",
      "tensor(2.4357, grad_fn=<NllLossBackward>)\n",
      "tensor(2.2911, grad_fn=<NllLossBackward>)\n",
      "tensor(2.1719, grad_fn=<NllLossBackward>)\n",
      "tensor(2.0683, grad_fn=<NllLossBackward>)\n",
      "tensor(1.9747, grad_fn=<NllLossBackward>)\n",
      "tensor(1.8880, grad_fn=<NllLossBackward>)\n",
      "tensor(1.8063, grad_fn=<NllLossBackward>)\n",
      "tensor(1.7284, grad_fn=<NllLossBackward>)\n",
      "tensor(1.6533, grad_fn=<NllLossBackward>)\n",
      "tensor(1.5805, grad_fn=<NllLossBackward>)\n",
      "tensor(1.5097, grad_fn=<NllLossBackward>)\n",
      "tensor(1.4405, grad_fn=<NllLossBackward>)\n",
      "tensor(1.3727, grad_fn=<NllLossBackward>)\n",
      "tensor(1.3063, grad_fn=<NllLossBackward>)\n",
      "tensor(1.2410, grad_fn=<NllLossBackward>)\n",
      "tensor(1.1770, grad_fn=<NllLossBackward>)\n",
      "tensor(1.1142, grad_fn=<NllLossBackward>)\n",
      "tensor(1.0527, grad_fn=<NllLossBackward>)\n",
      "tensor(0.9925, grad_fn=<NllLossBackward>)\n",
      "tensor(0.9337, grad_fn=<NllLossBackward>)\n",
      "tensor(0.8764, grad_fn=<NllLossBackward>)\n",
      "tensor(0.8208, grad_fn=<NllLossBackward>)\n",
      "tensor(0.7671, grad_fn=<NllLossBackward>)\n",
      "tensor(0.7154, grad_fn=<NllLossBackward>)\n",
      "tensor(0.6659, grad_fn=<NllLossBackward>)\n",
      "tensor(0.6188, grad_fn=<NllLossBackward>)\n",
      "tensor(0.5741, grad_fn=<NllLossBackward>)\n",
      "tensor(0.5321, grad_fn=<NllLossBackward>)\n",
      "tensor(0.4927, grad_fn=<NllLossBackward>)\n",
      "tensor(0.4561, grad_fn=<NllLossBackward>)\n",
      "tensor(0.4222, grad_fn=<NllLossBackward>)\n",
      "tensor(0.3909, grad_fn=<NllLossBackward>)\n",
      "tensor(0.3623, grad_fn=<NllLossBackward>)\n",
      "tensor(0.3362, grad_fn=<NllLossBackward>)\n",
      "tensor(0.3124, grad_fn=<NllLossBackward>)\n",
      "tensor(0.2909, grad_fn=<NllLossBackward>)\n",
      "tensor(0.2714, grad_fn=<NllLossBackward>)\n",
      "tensor(0.2537, grad_fn=<NllLossBackward>)\n",
      "tensor(0.2378, grad_fn=<NllLossBackward>)\n",
      "tensor(0.2234, grad_fn=<NllLossBackward>)\n",
      "tensor(0.2104, grad_fn=<NllLossBackward>)\n",
      "tensor(0.1987, grad_fn=<NllLossBackward>)\n",
      "tensor(0.1881, grad_fn=<NllLossBackward>)\n",
      "tensor(0.1784, grad_fn=<NllLossBackward>)\n",
      "tensor(0.1697, grad_fn=<NllLossBackward>)\n",
      "tensor(0.1618, grad_fn=<NllLossBackward>)\n",
      "tensor(0.1546, grad_fn=<NllLossBackward>)\n",
      "tensor(0.1481, grad_fn=<NllLossBackward>)\n",
      "tensor(0.1421, grad_fn=<NllLossBackward>)\n",
      "tensor(0.1367, grad_fn=<NllLossBackward>)\n",
      "tensor(0.1317, grad_fn=<NllLossBackward>)\n",
      "tensor(0.1271, grad_fn=<NllLossBackward>)\n",
      "tensor(0.1229, grad_fn=<NllLossBackward>)\n",
      "tensor(0.1190, grad_fn=<NllLossBackward>)\n",
      "tensor(0.1154, grad_fn=<NllLossBackward>)\n",
      "tensor(0.1121, grad_fn=<NllLossBackward>)\n",
      "tensor(0.1090, grad_fn=<NllLossBackward>)\n",
      "tensor(0.1062, grad_fn=<NllLossBackward>)\n",
      "tensor(0.1035, grad_fn=<NllLossBackward>)\n",
      "tensor(0.1011, grad_fn=<NllLossBackward>)\n",
      "tensor(0.0988, grad_fn=<NllLossBackward>)\n",
      "tensor(0.0966, grad_fn=<NllLossBackward>)\n",
      "tensor(0.0946, grad_fn=<NllLossBackward>)\n",
      "tensor(0.0927, grad_fn=<NllLossBackward>)\n",
      "tensor(0.0909, grad_fn=<NllLossBackward>)\n",
      "tensor(0.0893, grad_fn=<NllLossBackward>)\n",
      "tensor(0.0877, grad_fn=<NllLossBackward>)\n",
      "tensor(0.0862, grad_fn=<NllLossBackward>)\n",
      "tensor(0.0848, grad_fn=<NllLossBackward>)\n",
      "tensor(0.0835, grad_fn=<NllLossBackward>)\n",
      "tensor(0.0822, grad_fn=<NllLossBackward>)\n",
      "tensor(0.0810, grad_fn=<NllLossBackward>)\n",
      "tensor(0.0799, grad_fn=<NllLossBackward>)\n",
      "tensor(0.0788, grad_fn=<NllLossBackward>)\n",
      "tensor(0.0777, grad_fn=<NllLossBackward>)\n",
      "tensor(0.0767, grad_fn=<NllLossBackward>)\n",
      "tensor(0.0758, grad_fn=<NllLossBackward>)\n",
      "tensor(0.0749, grad_fn=<NllLossBackward>)\n",
      "tensor(0.0740, grad_fn=<NllLossBackward>)\n",
      "tensor(0.0732, grad_fn=<NllLossBackward>)\n",
      "tensor(0.0724, grad_fn=<NllLossBackward>)\n",
      "tensor(0.0716, grad_fn=<NllLossBackward>)\n",
      "tensor(0.0708, grad_fn=<NllLossBackward>)\n",
      "tensor(0.0701, grad_fn=<NllLossBackward>)\n",
      "tensor(0.0694, grad_fn=<NllLossBackward>)\n",
      "tensor(0.0687, grad_fn=<NllLossBackward>)\n",
      "tensor(0.0681, grad_fn=<NllLossBackward>)\n",
      "tensor(0.0674, grad_fn=<NllLossBackward>)\n",
      "tensor(0.0668, grad_fn=<NllLossBackward>)\n",
      "tensor(0.0662, grad_fn=<NllLossBackward>)\n",
      "tensor(0.0656, grad_fn=<NllLossBackward>)\n",
      "tensor(0.0650, grad_fn=<NllLossBackward>)\n",
      "tensor(0.0645, grad_fn=<NllLossBackward>)\n",
      "tensor(0.0639, grad_fn=<NllLossBackward>)\n",
      "tensor(0.0634, grad_fn=<NllLossBackward>)\n",
      "tensor(0.0629, grad_fn=<NllLossBackward>)\n",
      "tensor(0.0624, grad_fn=<NllLossBackward>)\n",
      "tensor(0.0619, grad_fn=<NllLossBackward>)\n",
      "tensor(0.0614, grad_fn=<NllLossBackward>)\n",
      "tensor(0.0610, grad_fn=<NllLossBackward>)\n",
      "tensor(0.0605, grad_fn=<NllLossBackward>)\n",
      "tensor(0.0601, grad_fn=<NllLossBackward>)\n",
      "tensor(0.0596, grad_fn=<NllLossBackward>)\n",
      "tensor(0.0592, grad_fn=<NllLossBackward>)\n",
      "tensor(0.0588, grad_fn=<NllLossBackward>)\n",
      "tensor(0.0584, grad_fn=<NllLossBackward>)\n",
      "tensor(0.0579, grad_fn=<NllLossBackward>)\n",
      "tensor(0.0575, grad_fn=<NllLossBackward>)\n",
      "tensor(0.0572, grad_fn=<NllLossBackward>)\n",
      "tensor(0.0568, grad_fn=<NllLossBackward>)\n",
      "tensor(0.0564, grad_fn=<NllLossBackward>)\n",
      "tensor(0.0560, grad_fn=<NllLossBackward>)\n",
      "tensor(0.0557, grad_fn=<NllLossBackward>)\n",
      "tensor(0.0553, grad_fn=<NllLossBackward>)\n",
      "tensor(0.0549, grad_fn=<NllLossBackward>)\n",
      "tensor(0.0546, grad_fn=<NllLossBackward>)\n",
      "tensor(0.0542, grad_fn=<NllLossBackward>)\n",
      "tensor(0.0539, grad_fn=<NllLossBackward>)\n",
      "tensor(0.0536, grad_fn=<NllLossBackward>)\n",
      "tensor(0.0533, grad_fn=<NllLossBackward>)\n",
      "tensor(0.0529, grad_fn=<NllLossBackward>)\n",
      "tensor(0.0526, grad_fn=<NllLossBackward>)\n",
      "tensor(0.0523, grad_fn=<NllLossBackward>)\n",
      "tensor(0.0520, grad_fn=<NllLossBackward>)\n",
      "tensor(0.0517, grad_fn=<NllLossBackward>)\n",
      "tensor(0.0514, grad_fn=<NllLossBackward>)\n",
      "tensor(0.0511, grad_fn=<NllLossBackward>)\n",
      "tensor(0.0508, grad_fn=<NllLossBackward>)\n",
      "tensor(0.0505, grad_fn=<NllLossBackward>)\n",
      "tensor(0.0502, grad_fn=<NllLossBackward>)\n",
      "tensor(0.0499, grad_fn=<NllLossBackward>)\n",
      "tensor(0.0497, grad_fn=<NllLossBackward>)\n",
      "tensor(0.0494, grad_fn=<NllLossBackward>)\n",
      "tensor(0.0491, grad_fn=<NllLossBackward>)\n",
      "tensor(0.0489, grad_fn=<NllLossBackward>)\n",
      "tensor(0.0486, grad_fn=<NllLossBackward>)\n",
      "tensor(0.0483, grad_fn=<NllLossBackward>)\n",
      "tensor(0.0481, grad_fn=<NllLossBackward>)\n",
      "tensor(0.0478, grad_fn=<NllLossBackward>)\n",
      "tensor(0.0476, grad_fn=<NllLossBackward>)\n",
      "tensor(0.0473, grad_fn=<NllLossBackward>)\n",
      "tensor(0.0471, grad_fn=<NllLossBackward>)\n",
      "tensor(0.0468, grad_fn=<NllLossBackward>)\n",
      "tensor(0.0466, grad_fn=<NllLossBackward>)\n",
      "tensor(0.0464, grad_fn=<NllLossBackward>)\n",
      "tensor(0.0461, grad_fn=<NllLossBackward>)\n",
      "tensor(0.0459, grad_fn=<NllLossBackward>)\n",
      "tensor(0.0457, grad_fn=<NllLossBackward>)\n",
      "tensor(0.0454, grad_fn=<NllLossBackward>)\n",
      "tensor(0.0452, grad_fn=<NllLossBackward>)\n",
      "tensor(0.0450, grad_fn=<NllLossBackward>)\n",
      "tensor(0.0448, grad_fn=<NllLossBackward>)\n",
      "tensor(0.0445, grad_fn=<NllLossBackward>)\n",
      "tensor(0.0443, grad_fn=<NllLossBackward>)\n",
      "tensor(0.0441, grad_fn=<NllLossBackward>)\n",
      "tensor(0.0439, grad_fn=<NllLossBackward>)\n",
      "tensor(0.0437, grad_fn=<NllLossBackward>)\n",
      "tensor(0.0435, grad_fn=<NllLossBackward>)\n",
      "tensor(0.0433, grad_fn=<NllLossBackward>)\n",
      "tensor(0.0431, grad_fn=<NllLossBackward>)\n",
      "tensor(0.0429, grad_fn=<NllLossBackward>)\n",
      "tensor(0.0427, grad_fn=<NllLossBackward>)\n",
      "tensor(0.0425, grad_fn=<NllLossBackward>)\n",
      "tensor(0.0423, grad_fn=<NllLossBackward>)\n",
      "tensor(0.0421, grad_fn=<NllLossBackward>)\n",
      "tensor(0.0419, grad_fn=<NllLossBackward>)\n",
      "tensor(0.0417, grad_fn=<NllLossBackward>)\n",
      "tensor(0.0415, grad_fn=<NllLossBackward>)\n",
      "tensor(0.0414, grad_fn=<NllLossBackward>)\n",
      "tensor(0.0412, grad_fn=<NllLossBackward>)\n",
      "tensor(0.0410, grad_fn=<NllLossBackward>)\n",
      "tensor(0.0408, grad_fn=<NllLossBackward>)\n",
      "tensor(0.0406, grad_fn=<NllLossBackward>)\n",
      "tensor(0.0405, grad_fn=<NllLossBackward>)\n",
      "tensor(0.0403, grad_fn=<NllLossBackward>)\n",
      "tensor(0.0401, grad_fn=<NllLossBackward>)\n",
      "tensor(0.0399, grad_fn=<NllLossBackward>)\n",
      "tensor(0.0398, grad_fn=<NllLossBackward>)\n",
      "tensor(0.0396, grad_fn=<NllLossBackward>)\n",
      "tensor(0.0394, grad_fn=<NllLossBackward>)\n",
      "tensor(0.0393, grad_fn=<NllLossBackward>)\n",
      "tensor(0.0391, grad_fn=<NllLossBackward>)\n",
      "tensor(0.0389, grad_fn=<NllLossBackward>)\n",
      "tensor(0.0388, grad_fn=<NllLossBackward>)\n",
      "tensor(0.0386, grad_fn=<NllLossBackward>)\n",
      "tensor(0.0385, grad_fn=<NllLossBackward>)\n",
      "tensor(0.0383, grad_fn=<NllLossBackward>)\n",
      "tensor(0.0382, grad_fn=<NllLossBackward>)\n",
      "tensor(0.0380, grad_fn=<NllLossBackward>)\n",
      "tensor(0.0378, grad_fn=<NllLossBackward>)\n",
      "tensor(0.0377, grad_fn=<NllLossBackward>)\n",
      "tensor(0.0375, grad_fn=<NllLossBackward>)\n",
      "tensor(0.0374, grad_fn=<NllLossBackward>)\n",
      "tensor(0.0373, grad_fn=<NllLossBackward>)\n",
      "tensor(0.0371, grad_fn=<NllLossBackward>)\n",
      "tensor(0.0370, grad_fn=<NllLossBackward>)\n",
      "tensor(0.0368, grad_fn=<NllLossBackward>)\n",
      "tensor(0.0367, grad_fn=<NllLossBackward>)\n",
      "tensor(0.0365, grad_fn=<NllLossBackward>)\n",
      "tensor(0.0364, grad_fn=<NllLossBackward>)\n",
      "tensor(0.0363, grad_fn=<NllLossBackward>)\n",
      "tensor(0.0361, grad_fn=<NllLossBackward>)\n",
      "tensor(0.0360, grad_fn=<NllLossBackward>)\n",
      "tensor(0.0358, grad_fn=<NllLossBackward>)\n",
      "tensor(0.0357, grad_fn=<NllLossBackward>)\n",
      "tensor(0.0356, grad_fn=<NllLossBackward>)\n",
      "tensor(0.0354, grad_fn=<NllLossBackward>)\n",
      "tensor(0.0353, grad_fn=<NllLossBackward>)\n",
      "tensor(0.0352, grad_fn=<NllLossBackward>)\n",
      "tensor(0.0350, grad_fn=<NllLossBackward>)\n",
      "tensor(0.0349, grad_fn=<NllLossBackward>)\n",
      "tensor(0.0348, grad_fn=<NllLossBackward>)\n",
      "tensor(0.0347, grad_fn=<NllLossBackward>)\n",
      "tensor(0.0345, grad_fn=<NllLossBackward>)\n",
      "tensor(0.0344, grad_fn=<NllLossBackward>)\n",
      "tensor(0.0343, grad_fn=<NllLossBackward>)\n",
      "tensor(0.0342, grad_fn=<NllLossBackward>)\n",
      "tensor(0.0340, grad_fn=<NllLossBackward>)\n",
      "tensor(0.0339, grad_fn=<NllLossBackward>)\n",
      "tensor(0.0338, grad_fn=<NllLossBackward>)\n",
      "tensor(0.0337, grad_fn=<NllLossBackward>)\n",
      "tensor(0.0336, grad_fn=<NllLossBackward>)\n",
      "tensor(0.0334, grad_fn=<NllLossBackward>)\n",
      "tensor(0.0333, grad_fn=<NllLossBackward>)\n",
      "tensor(0.0332, grad_fn=<NllLossBackward>)\n",
      "tensor(0.0331, grad_fn=<NllLossBackward>)\n",
      "tensor(0.0330, grad_fn=<NllLossBackward>)\n",
      "tensor(0.0329, grad_fn=<NllLossBackward>)\n",
      "tensor(0.0328, grad_fn=<NllLossBackward>)\n",
      "tensor(0.0326, grad_fn=<NllLossBackward>)\n",
      "tensor(0.0325, grad_fn=<NllLossBackward>)\n",
      "tensor(0.0324, grad_fn=<NllLossBackward>)\n",
      "tensor(0.0323, grad_fn=<NllLossBackward>)\n",
      "tensor(0.0322, grad_fn=<NllLossBackward>)\n",
      "tensor(0.0321, grad_fn=<NllLossBackward>)\n",
      "tensor(0.0320, grad_fn=<NllLossBackward>)\n",
      "tensor(0.0319, grad_fn=<NllLossBackward>)\n",
      "tensor(0.0318, grad_fn=<NllLossBackward>)\n",
      "tensor(0.0317, grad_fn=<NllLossBackward>)\n",
      "tensor(0.0316, grad_fn=<NllLossBackward>)\n",
      "tensor(0.0315, grad_fn=<NllLossBackward>)\n",
      "tensor(0.0314, grad_fn=<NllLossBackward>)\n",
      "tensor(0.0313, grad_fn=<NllLossBackward>)\n",
      "tensor(0.0312, grad_fn=<NllLossBackward>)\n",
      "tensor(0.0311, grad_fn=<NllLossBackward>)\n",
      "tensor(0.0310, grad_fn=<NllLossBackward>)\n",
      "tensor(0.0309, grad_fn=<NllLossBackward>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.0308, grad_fn=<NllLossBackward>)\n",
      "tensor(0.0307, grad_fn=<NllLossBackward>)\n",
      "tensor(0.0306, grad_fn=<NllLossBackward>)\n",
      "tensor(0.0305, grad_fn=<NllLossBackward>)\n",
      "tensor(0.0304, grad_fn=<NllLossBackward>)\n",
      "tensor(0.0303, grad_fn=<NllLossBackward>)\n",
      "tensor(0.0302, grad_fn=<NllLossBackward>)\n",
      "tensor(0.0301, grad_fn=<NllLossBackward>)\n",
      "tensor(0.0300, grad_fn=<NllLossBackward>)\n",
      "tensor(0.0299, grad_fn=<NllLossBackward>)\n",
      "tensor(0.0298, grad_fn=<NllLossBackward>)\n",
      "tensor(0.0297, grad_fn=<NllLossBackward>)\n",
      "tensor(0.0296, grad_fn=<NllLossBackward>)\n",
      "tensor(0.0295, grad_fn=<NllLossBackward>)\n",
      "tensor(0.0295, grad_fn=<NllLossBackward>)\n",
      "tensor(0.0294, grad_fn=<NllLossBackward>)\n",
      "tensor(0.0293, grad_fn=<NllLossBackward>)\n",
      "tensor(0.0292, grad_fn=<NllLossBackward>)\n",
      "tensor(0.0291, grad_fn=<NllLossBackward>)\n",
      "tensor(0.0290, grad_fn=<NllLossBackward>)\n",
      "tensor(0.0289, grad_fn=<NllLossBackward>)\n",
      "tensor(0.0288, grad_fn=<NllLossBackward>)\n",
      "tensor(0.0288, grad_fn=<NllLossBackward>)\n",
      "tensor(0.0287, grad_fn=<NllLossBackward>)\n",
      "tensor(0.0286, grad_fn=<NllLossBackward>)\n",
      "tensor(0.0285, grad_fn=<NllLossBackward>)\n",
      "tensor(0.0284, grad_fn=<NllLossBackward>)\n",
      "tensor(0.0283, grad_fn=<NllLossBackward>)\n",
      "tensor(0.0282, grad_fn=<NllLossBackward>)\n",
      "tensor(0.0282, grad_fn=<NllLossBackward>)\n",
      "tensor(0.0281, grad_fn=<NllLossBackward>)\n",
      "tensor(0.0280, grad_fn=<NllLossBackward>)\n",
      "tensor(0.0279, grad_fn=<NllLossBackward>)\n",
      "tensor(0.0278, grad_fn=<NllLossBackward>)\n",
      "tensor(0.0278, grad_fn=<NllLossBackward>)\n",
      "tensor(0.0277, grad_fn=<NllLossBackward>)\n",
      "tensor(0.0276, grad_fn=<NllLossBackward>)\n",
      "tensor(0.0275, grad_fn=<NllLossBackward>)\n",
      "tensor(0.0275, grad_fn=<NllLossBackward>)\n",
      "tensor(0.0274, grad_fn=<NllLossBackward>)\n",
      "tensor(0.0273, grad_fn=<NllLossBackward>)\n",
      "tensor(0.0272, grad_fn=<NllLossBackward>)\n",
      "tensor(0.0271, grad_fn=<NllLossBackward>)\n",
      "tensor(0.0271, grad_fn=<NllLossBackward>)\n",
      "tensor(0.0270, grad_fn=<NllLossBackward>)\n",
      "tensor(0.0269, grad_fn=<NllLossBackward>)\n",
      "tensor(0.0268, grad_fn=<NllLossBackward>)\n",
      "tensor(0.0268, grad_fn=<NllLossBackward>)\n",
      "tensor(0.0267, grad_fn=<NllLossBackward>)\n",
      "tensor(0.0266, grad_fn=<NllLossBackward>)\n",
      "tensor(0.0266, grad_fn=<NllLossBackward>)\n",
      "tensor(0.0265, grad_fn=<NllLossBackward>)\n",
      "tensor(0.0264, grad_fn=<NllLossBackward>)\n",
      "tensor(0.0263, grad_fn=<NllLossBackward>)\n",
      "tensor(0.0263, grad_fn=<NllLossBackward>)\n",
      "tensor(0.0262, grad_fn=<NllLossBackward>)\n",
      "tensor(0.0261, grad_fn=<NllLossBackward>)\n",
      "tensor(0.0261, grad_fn=<NllLossBackward>)\n",
      "tensor(0.0260, grad_fn=<NllLossBackward>)\n",
      "tensor(0.0259, grad_fn=<NllLossBackward>)\n",
      "tensor(0.0258, grad_fn=<NllLossBackward>)\n",
      "tensor(0.0258, grad_fn=<NllLossBackward>)\n",
      "tensor(0.0257, grad_fn=<NllLossBackward>)\n",
      "tensor(0.0256, grad_fn=<NllLossBackward>)\n",
      "tensor(0.0256, grad_fn=<NllLossBackward>)\n",
      "tensor(0.0255, grad_fn=<NllLossBackward>)\n",
      "tensor(0.0254, grad_fn=<NllLossBackward>)\n",
      "tensor(0.0254, grad_fn=<NllLossBackward>)\n",
      "tensor(0.0253, grad_fn=<NllLossBackward>)\n",
      "tensor(0.0252, grad_fn=<NllLossBackward>)\n",
      "tensor(0.0252, grad_fn=<NllLossBackward>)\n",
      "tensor(0.0251, grad_fn=<NllLossBackward>)\n",
      "tensor(0.0251, grad_fn=<NllLossBackward>)\n",
      "tensor(0.0250, grad_fn=<NllLossBackward>)\n",
      "tensor(0.0249, grad_fn=<NllLossBackward>)\n",
      "tensor(0.0249, grad_fn=<NllLossBackward>)\n",
      "tensor(0.0248, grad_fn=<NllLossBackward>)\n",
      "tensor(0.0247, grad_fn=<NllLossBackward>)\n",
      "tensor(0.0247, grad_fn=<NllLossBackward>)\n",
      "tensor(0.0246, grad_fn=<NllLossBackward>)\n",
      "tensor(0.0246, grad_fn=<NllLossBackward>)\n",
      "tensor(0.0245, grad_fn=<NllLossBackward>)\n",
      "tensor(0.0244, grad_fn=<NllLossBackward>)\n",
      "tensor(0.0244, grad_fn=<NllLossBackward>)\n",
      "tensor(0.0243, grad_fn=<NllLossBackward>)\n",
      "tensor(0.0243, grad_fn=<NllLossBackward>)\n",
      "tensor(0.0242, grad_fn=<NllLossBackward>)\n",
      "tensor(0.0241, grad_fn=<NllLossBackward>)\n",
      "tensor(0.0241, grad_fn=<NllLossBackward>)\n",
      "tensor(0.0240, grad_fn=<NllLossBackward>)\n",
      "tensor(0.0240, grad_fn=<NllLossBackward>)\n",
      "tensor(0.0239, grad_fn=<NllLossBackward>)\n",
      "tensor(0.0238, grad_fn=<NllLossBackward>)\n",
      "tensor(0.0238, grad_fn=<NllLossBackward>)\n",
      "tensor(0.0237, grad_fn=<NllLossBackward>)\n",
      "tensor(0.0237, grad_fn=<NllLossBackward>)\n",
      "tensor(0.0236, grad_fn=<NllLossBackward>)\n",
      "tensor(0.0236, grad_fn=<NllLossBackward>)\n",
      "tensor(0.0235, grad_fn=<NllLossBackward>)\n",
      "tensor(0.0234, grad_fn=<NllLossBackward>)\n",
      "tensor(0.0234, grad_fn=<NllLossBackward>)\n",
      "tensor(0.0233, grad_fn=<NllLossBackward>)\n",
      "tensor(0.0233, grad_fn=<NllLossBackward>)\n",
      "tensor(0.0232, grad_fn=<NllLossBackward>)\n",
      "tensor(0.0232, grad_fn=<NllLossBackward>)\n",
      "tensor(0.0231, grad_fn=<NllLossBackward>)\n",
      "tensor(0.0231, grad_fn=<NllLossBackward>)\n",
      "tensor(0.0230, grad_fn=<NllLossBackward>)\n",
      "tensor(0.0229, grad_fn=<NllLossBackward>)\n",
      "tensor(0.0229, grad_fn=<NllLossBackward>)\n",
      "tensor(0.0228, grad_fn=<NllLossBackward>)\n",
      "tensor(0.0228, grad_fn=<NllLossBackward>)\n",
      "tensor(0.0227, grad_fn=<NllLossBackward>)\n",
      "tensor(0.0227, grad_fn=<NllLossBackward>)\n",
      "tensor(0.0226, grad_fn=<NllLossBackward>)\n",
      "tensor(0.0226, grad_fn=<NllLossBackward>)\n",
      "tensor(0.0225, grad_fn=<NllLossBackward>)\n",
      "tensor(0.0225, grad_fn=<NllLossBackward>)\n",
      "tensor(0.0224, grad_fn=<NllLossBackward>)\n",
      "tensor(0.0224, grad_fn=<NllLossBackward>)\n",
      "tensor(0.0223, grad_fn=<NllLossBackward>)\n",
      "tensor(0.0223, grad_fn=<NllLossBackward>)\n",
      "tensor(0.0222, grad_fn=<NllLossBackward>)\n",
      "tensor(0.0222, grad_fn=<NllLossBackward>)\n",
      "tensor(0.0221, grad_fn=<NllLossBackward>)\n",
      "tensor(0.0221, grad_fn=<NllLossBackward>)\n",
      "tensor(0.0220, grad_fn=<NllLossBackward>)\n",
      "tensor(0.0220, grad_fn=<NllLossBackward>)\n",
      "tensor(0.0219, grad_fn=<NllLossBackward>)\n",
      "tensor(0.0219, grad_fn=<NllLossBackward>)\n",
      "tensor(0.0218, grad_fn=<NllLossBackward>)\n",
      "tensor(0.0218, grad_fn=<NllLossBackward>)\n",
      "tensor(0.0217, grad_fn=<NllLossBackward>)\n",
      "tensor(0.0217, grad_fn=<NllLossBackward>)\n",
      "tensor(0.0216, grad_fn=<NllLossBackward>)\n",
      "tensor(0.0216, grad_fn=<NllLossBackward>)\n",
      "tensor(0.0215, grad_fn=<NllLossBackward>)\n",
      "tensor(0.0215, grad_fn=<NllLossBackward>)\n",
      "tensor(0.0215, grad_fn=<NllLossBackward>)\n",
      "tensor(0.0214, grad_fn=<NllLossBackward>)\n",
      "tensor(0.0214, grad_fn=<NllLossBackward>)\n",
      "tensor(0.0213, grad_fn=<NllLossBackward>)\n",
      "tensor(0.0213, grad_fn=<NllLossBackward>)\n",
      "tensor(0.0212, grad_fn=<NllLossBackward>)\n",
      "tensor(0.0212, grad_fn=<NllLossBackward>)\n",
      "tensor(0.0211, grad_fn=<NllLossBackward>)\n",
      "tensor(0.0211, grad_fn=<NllLossBackward>)\n",
      "tensor(0.0210, grad_fn=<NllLossBackward>)\n",
      "tensor(0.0210, grad_fn=<NllLossBackward>)\n",
      "tensor(0.0210, grad_fn=<NllLossBackward>)\n",
      "tensor(0.0209, grad_fn=<NllLossBackward>)\n",
      "tensor(0.0209, grad_fn=<NllLossBackward>)\n",
      "tensor(0.0208, grad_fn=<NllLossBackward>)\n",
      "tensor(0.0208, grad_fn=<NllLossBackward>)\n",
      "tensor(0.0207, grad_fn=<NllLossBackward>)\n",
      "tensor(0.0207, grad_fn=<NllLossBackward>)\n",
      "tensor(0.0206, grad_fn=<NllLossBackward>)\n",
      "tensor(0.0206, grad_fn=<NllLossBackward>)\n",
      "tensor(0.0206, grad_fn=<NllLossBackward>)\n",
      "tensor(0.0205, grad_fn=<NllLossBackward>)\n",
      "tensor(0.0205, grad_fn=<NllLossBackward>)\n",
      "tensor(0.0204, grad_fn=<NllLossBackward>)\n",
      "tensor(0.0204, grad_fn=<NllLossBackward>)\n",
      "tensor(0.0204, grad_fn=<NllLossBackward>)\n",
      "tensor(0.0203, grad_fn=<NllLossBackward>)\n",
      "tensor(0.0203, grad_fn=<NllLossBackward>)\n",
      "tensor(0.0202, grad_fn=<NllLossBackward>)\n",
      "tensor(0.0202, grad_fn=<NllLossBackward>)\n",
      "tensor(0.0201, grad_fn=<NllLossBackward>)\n",
      "tensor(0.0201, grad_fn=<NllLossBackward>)\n",
      "tensor(0.0201, grad_fn=<NllLossBackward>)\n",
      "tensor(0.0200, grad_fn=<NllLossBackward>)\n",
      "tensor(0.0200, grad_fn=<NllLossBackward>)\n",
      "tensor(0.0199, grad_fn=<NllLossBackward>)\n",
      "tensor(0.0199, grad_fn=<NllLossBackward>)\n",
      "tensor(0.0199, grad_fn=<NllLossBackward>)\n",
      "tensor(0.0198, grad_fn=<NllLossBackward>)\n",
      "tensor(0.0198, grad_fn=<NllLossBackward>)\n",
      "tensor(0.0197, grad_fn=<NllLossBackward>)\n",
      "tensor(0.0197, grad_fn=<NllLossBackward>)\n",
      "tensor(0.0197, grad_fn=<NllLossBackward>)\n",
      "tensor(0.0196, grad_fn=<NllLossBackward>)\n",
      "tensor(0.0196, grad_fn=<NllLossBackward>)\n",
      "tensor(0.0196, grad_fn=<NllLossBackward>)\n",
      "tensor(0.0195, grad_fn=<NllLossBackward>)\n",
      "tensor(0.0195, grad_fn=<NllLossBackward>)\n",
      "tensor(0.0194, grad_fn=<NllLossBackward>)\n",
      "tensor(0.0194, grad_fn=<NllLossBackward>)\n",
      "tensor(0.0194, grad_fn=<NllLossBackward>)\n",
      "tensor(0.0193, grad_fn=<NllLossBackward>)\n",
      "tensor(0.0193, grad_fn=<NllLossBackward>)\n",
      "tensor(0.0193, grad_fn=<NllLossBackward>)\n",
      "tensor(0.0192, grad_fn=<NllLossBackward>)\n",
      "tensor(0.0192, grad_fn=<NllLossBackward>)\n",
      "tensor(0.0191, grad_fn=<NllLossBackward>)\n",
      "tensor(0.0191, grad_fn=<NllLossBackward>)\n",
      "tensor(0.0191, grad_fn=<NllLossBackward>)\n",
      "tensor(0.0190, grad_fn=<NllLossBackward>)\n",
      "tensor(0.0190, grad_fn=<NllLossBackward>)\n",
      "tensor(0.0190, grad_fn=<NllLossBackward>)\n",
      "tensor(0.0189, grad_fn=<NllLossBackward>)\n",
      "tensor(0.0189, grad_fn=<NllLossBackward>)\n",
      "tensor(0.0189, grad_fn=<NllLossBackward>)\n",
      "tensor(0.0188, grad_fn=<NllLossBackward>)\n",
      "tensor(0.0188, grad_fn=<NllLossBackward>)\n",
      "tensor(0.0187, grad_fn=<NllLossBackward>)\n",
      "tensor(0.0187, grad_fn=<NllLossBackward>)\n",
      "tensor(0.0187, grad_fn=<NllLossBackward>)\n",
      "tensor(0.0186, grad_fn=<NllLossBackward>)\n",
      "tensor(0.0186, grad_fn=<NllLossBackward>)\n",
      "tensor(0.0186, grad_fn=<NllLossBackward>)\n",
      "tensor(0.0185, grad_fn=<NllLossBackward>)\n",
      "tensor(0.0185, grad_fn=<NllLossBackward>)\n",
      "tensor(0.0185, grad_fn=<NllLossBackward>)\n",
      "tensor(0.0184, grad_fn=<NllLossBackward>)\n",
      "tensor(0.0184, grad_fn=<NllLossBackward>)\n",
      "tensor(0.0184, grad_fn=<NllLossBackward>)\n",
      "tensor(0.0183, grad_fn=<NllLossBackward>)\n",
      "tensor(0.0183, grad_fn=<NllLossBackward>)\n",
      "tensor(0.0183, grad_fn=<NllLossBackward>)\n",
      "tensor(0.0182, grad_fn=<NllLossBackward>)\n",
      "tensor(0.0182, grad_fn=<NllLossBackward>)\n",
      "tensor(0.0182, grad_fn=<NllLossBackward>)\n",
      "tensor(0.0181, grad_fn=<NllLossBackward>)\n",
      "tensor(0.0181, grad_fn=<NllLossBackward>)\n",
      "tensor(0.0181, grad_fn=<NllLossBackward>)\n",
      "tensor(0.0180, grad_fn=<NllLossBackward>)\n",
      "tensor(0.0180, grad_fn=<NllLossBackward>)\n",
      "tensor(0.0180, grad_fn=<NllLossBackward>)\n",
      "tensor(0.0179, grad_fn=<NllLossBackward>)\n",
      "tensor(0.0179, grad_fn=<NllLossBackward>)\n",
      "tensor(0.0179, grad_fn=<NllLossBackward>)\n",
      "tensor(0.0178, grad_fn=<NllLossBackward>)\n",
      "tensor(0.0178, grad_fn=<NllLossBackward>)\n",
      "tensor(0.0178, grad_fn=<NllLossBackward>)\n",
      "tensor(0.0177, grad_fn=<NllLossBackward>)\n",
      "tensor(0.0177, grad_fn=<NllLossBackward>)\n",
      "tensor(0.0177, grad_fn=<NllLossBackward>)\n",
      "tensor(0.0177, grad_fn=<NllLossBackward>)\n",
      "tensor(0.0176, grad_fn=<NllLossBackward>)\n",
      "tensor(0.0176, grad_fn=<NllLossBackward>)\n",
      "tensor(0.0176, grad_fn=<NllLossBackward>)\n",
      "tensor(0.0175, grad_fn=<NllLossBackward>)\n",
      "tensor(0.0175, grad_fn=<NllLossBackward>)\n",
      "tensor(0.0175, grad_fn=<NllLossBackward>)\n",
      "tensor(0.0174, grad_fn=<NllLossBackward>)\n",
      "tensor(0.0174, grad_fn=<NllLossBackward>)\n",
      "tensor(0.0174, grad_fn=<NllLossBackward>)\n",
      "tensor(0.0173, grad_fn=<NllLossBackward>)\n",
      "tensor(0.0173, grad_fn=<NllLossBackward>)\n",
      "tensor(0.0173, grad_fn=<NllLossBackward>)\n",
      "tensor(0.0173, grad_fn=<NllLossBackward>)\n",
      "tensor(0.0172, grad_fn=<NllLossBackward>)\n",
      "tensor(0.0172, grad_fn=<NllLossBackward>)\n",
      "tensor(0.0172, grad_fn=<NllLossBackward>)\n",
      "tensor(0.0171, grad_fn=<NllLossBackward>)\n",
      "tensor(0.0171, grad_fn=<NllLossBackward>)\n",
      "tensor(0.0171, grad_fn=<NllLossBackward>)\n",
      "tensor(0.0171, grad_fn=<NllLossBackward>)\n",
      "tensor(0.0170, grad_fn=<NllLossBackward>)\n",
      "tensor(0.0170, grad_fn=<NllLossBackward>)\n",
      "tensor(0.0170, grad_fn=<NllLossBackward>)\n",
      "tensor(0.0169, grad_fn=<NllLossBackward>)\n",
      "tensor(0.0169, grad_fn=<NllLossBackward>)\n",
      "tensor(0.0169, grad_fn=<NllLossBackward>)\n",
      "tensor(0.0169, grad_fn=<NllLossBackward>)\n",
      "tensor(0.0168, grad_fn=<NllLossBackward>)\n",
      "tensor(0.0168, grad_fn=<NllLossBackward>)\n",
      "tensor(0.0168, grad_fn=<NllLossBackward>)\n",
      "tensor(0.0167, grad_fn=<NllLossBackward>)\n",
      "tensor(0.0167, grad_fn=<NllLossBackward>)\n",
      "tensor(0.0167, grad_fn=<NllLossBackward>)\n",
      "tensor(0.0167, grad_fn=<NllLossBackward>)\n",
      "tensor(0.0166, grad_fn=<NllLossBackward>)\n",
      "tensor(0.0166, grad_fn=<NllLossBackward>)\n",
      "tensor(0.0166, grad_fn=<NllLossBackward>)\n",
      "tensor(0.0165, grad_fn=<NllLossBackward>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.0165, grad_fn=<NllLossBackward>)\n",
      "tensor(0.0165, grad_fn=<NllLossBackward>)\n",
      "tensor(0.0165, grad_fn=<NllLossBackward>)\n",
      "tensor(0.0164, grad_fn=<NllLossBackward>)\n",
      "tensor(0.0164, grad_fn=<NllLossBackward>)\n",
      "tensor(0.0164, grad_fn=<NllLossBackward>)\n",
      "tensor(0.0164, grad_fn=<NllLossBackward>)\n",
      "tensor(0.0163, grad_fn=<NllLossBackward>)\n",
      "tensor(0.0163, grad_fn=<NllLossBackward>)\n",
      "tensor(0.0163, grad_fn=<NllLossBackward>)\n",
      "tensor(0.0163, grad_fn=<NllLossBackward>)\n",
      "tensor(0.0162, grad_fn=<NllLossBackward>)\n",
      "tensor(0.0162, grad_fn=<NllLossBackward>)\n",
      "tensor(0.0162, grad_fn=<NllLossBackward>)\n",
      "tensor(0.0161, grad_fn=<NllLossBackward>)\n",
      "tensor(0.0161, grad_fn=<NllLossBackward>)\n",
      "tensor(0.0161, grad_fn=<NllLossBackward>)\n",
      "tensor(0.0161, grad_fn=<NllLossBackward>)\n",
      "tensor(0.0160, grad_fn=<NllLossBackward>)\n",
      "tensor(0.0160, grad_fn=<NllLossBackward>)\n",
      "tensor(0.0160, grad_fn=<NllLossBackward>)\n",
      "tensor(0.0160, grad_fn=<NllLossBackward>)\n",
      "tensor(0.0159, grad_fn=<NllLossBackward>)\n",
      "tensor(0.0159, grad_fn=<NllLossBackward>)\n",
      "tensor(0.0159, grad_fn=<NllLossBackward>)\n",
      "tensor(0.0159, grad_fn=<NllLossBackward>)\n",
      "tensor(0.0158, grad_fn=<NllLossBackward>)\n",
      "tensor(0.0158, grad_fn=<NllLossBackward>)\n",
      "tensor(0.0158, grad_fn=<NllLossBackward>)\n",
      "tensor(0.0158, grad_fn=<NllLossBackward>)\n",
      "tensor(0.0157, grad_fn=<NllLossBackward>)\n",
      "tensor(0.0157, grad_fn=<NllLossBackward>)\n",
      "tensor(0.0157, grad_fn=<NllLossBackward>)\n",
      "tensor(0.0157, grad_fn=<NllLossBackward>)\n",
      "tensor(0.0156, grad_fn=<NllLossBackward>)\n",
      "tensor(0.0156, grad_fn=<NllLossBackward>)\n",
      "tensor(0.0156, grad_fn=<NllLossBackward>)\n",
      "tensor(0.0156, grad_fn=<NllLossBackward>)\n",
      "tensor(0.0156, grad_fn=<NllLossBackward>)\n",
      "tensor(0.0155, grad_fn=<NllLossBackward>)\n",
      "tensor(0.0155, grad_fn=<NllLossBackward>)\n",
      "tensor(0.0155, grad_fn=<NllLossBackward>)\n",
      "tensor(0.0155, grad_fn=<NllLossBackward>)\n",
      "tensor(0.0154, grad_fn=<NllLossBackward>)\n",
      "tensor(0.0154, grad_fn=<NllLossBackward>)\n",
      "tensor(0.0154, grad_fn=<NllLossBackward>)\n",
      "tensor(0.0154, grad_fn=<NllLossBackward>)\n",
      "tensor(0.0153, grad_fn=<NllLossBackward>)\n",
      "tensor(0.0153, grad_fn=<NllLossBackward>)\n",
      "tensor(0.0153, grad_fn=<NllLossBackward>)\n",
      "tensor(0.0153, grad_fn=<NllLossBackward>)\n",
      "tensor(0.0152, grad_fn=<NllLossBackward>)\n",
      "tensor(0.0152, grad_fn=<NllLossBackward>)\n",
      "tensor(0.0152, grad_fn=<NllLossBackward>)\n",
      "tensor(0.0152, grad_fn=<NllLossBackward>)\n",
      "tensor(0.0152, grad_fn=<NllLossBackward>)\n",
      "tensor(0.0151, grad_fn=<NllLossBackward>)\n",
      "tensor(0.0151, grad_fn=<NllLossBackward>)\n",
      "tensor(0.0151, grad_fn=<NllLossBackward>)\n",
      "tensor(0.0151, grad_fn=<NllLossBackward>)\n",
      "tensor(0.0150, grad_fn=<NllLossBackward>)\n",
      "tensor(0.0150, grad_fn=<NllLossBackward>)\n",
      "tensor(0.0150, grad_fn=<NllLossBackward>)\n",
      "tensor(0.0150, grad_fn=<NllLossBackward>)\n",
      "tensor(0.0150, grad_fn=<NllLossBackward>)\n",
      "tensor(0.0149, grad_fn=<NllLossBackward>)\n",
      "tensor(0.0149, grad_fn=<NllLossBackward>)\n",
      "tensor(0.0149, grad_fn=<NllLossBackward>)\n",
      "tensor(0.0149, grad_fn=<NllLossBackward>)\n",
      "tensor(0.0148, grad_fn=<NllLossBackward>)\n",
      "tensor(0.0148, grad_fn=<NllLossBackward>)\n",
      "tensor(0.0148, grad_fn=<NllLossBackward>)\n",
      "tensor(0.0148, grad_fn=<NllLossBackward>)\n",
      "tensor(0.0148, grad_fn=<NllLossBackward>)\n",
      "tensor(0.0147, grad_fn=<NllLossBackward>)\n",
      "tensor(0.0147, grad_fn=<NllLossBackward>)\n",
      "tensor(0.0147, grad_fn=<NllLossBackward>)\n",
      "tensor(0.0147, grad_fn=<NllLossBackward>)\n",
      "tensor(0.0147, grad_fn=<NllLossBackward>)\n",
      "tensor(0.0146, grad_fn=<NllLossBackward>)\n",
      "tensor(0.0146, grad_fn=<NllLossBackward>)\n",
      "tensor(0.0146, grad_fn=<NllLossBackward>)\n",
      "tensor(0.0146, grad_fn=<NllLossBackward>)\n",
      "tensor(0.0145, grad_fn=<NllLossBackward>)\n",
      "tensor(0.0145, grad_fn=<NllLossBackward>)\n",
      "tensor(0.0145, grad_fn=<NllLossBackward>)\n",
      "tensor(0.0145, grad_fn=<NllLossBackward>)\n",
      "tensor(0.0145, grad_fn=<NllLossBackward>)\n",
      "tensor(0.0144, grad_fn=<NllLossBackward>)\n",
      "tensor(0.0144, grad_fn=<NllLossBackward>)\n",
      "tensor(0.0144, grad_fn=<NllLossBackward>)\n",
      "tensor(0.0144, grad_fn=<NllLossBackward>)\n",
      "tensor(0.0144, grad_fn=<NllLossBackward>)\n",
      "tensor(0.0143, grad_fn=<NllLossBackward>)\n",
      "tensor(0.0143, grad_fn=<NllLossBackward>)\n",
      "tensor(0.0143, grad_fn=<NllLossBackward>)\n",
      "tensor(0.0143, grad_fn=<NllLossBackward>)\n",
      "tensor(0.0143, grad_fn=<NllLossBackward>)\n",
      "tensor(0.0142, grad_fn=<NllLossBackward>)\n",
      "tensor(0.0142, grad_fn=<NllLossBackward>)\n",
      "tensor(0.0142, grad_fn=<NllLossBackward>)\n",
      "tensor(0.0142, grad_fn=<NllLossBackward>)\n",
      "tensor(0.0142, grad_fn=<NllLossBackward>)\n",
      "tensor(0.0141, grad_fn=<NllLossBackward>)\n",
      "tensor(0.0141, grad_fn=<NllLossBackward>)\n",
      "tensor(0.0141, grad_fn=<NllLossBackward>)\n",
      "tensor(0.0141, grad_fn=<NllLossBackward>)\n",
      "tensor(0.0141, grad_fn=<NllLossBackward>)\n",
      "tensor(0.0140, grad_fn=<NllLossBackward>)\n",
      "tensor(0.0140, grad_fn=<NllLossBackward>)\n",
      "tensor(0.0140, grad_fn=<NllLossBackward>)\n",
      "tensor(0.0140, grad_fn=<NllLossBackward>)\n",
      "tensor(0.0140, grad_fn=<NllLossBackward>)\n",
      "tensor(0.0140, grad_fn=<NllLossBackward>)\n",
      "tensor(0.0139, grad_fn=<NllLossBackward>)\n",
      "tensor(0.0139, grad_fn=<NllLossBackward>)\n",
      "tensor(0.0139, grad_fn=<NllLossBackward>)\n",
      "tensor(0.0139, grad_fn=<NllLossBackward>)\n",
      "tensor(0.0139, grad_fn=<NllLossBackward>)\n",
      "tensor(0.0138, grad_fn=<NllLossBackward>)\n",
      "tensor(0.0138, grad_fn=<NllLossBackward>)\n",
      "tensor(0.0138, grad_fn=<NllLossBackward>)\n",
      "tensor(0.0138, grad_fn=<NllLossBackward>)\n",
      "tensor(0.0138, grad_fn=<NllLossBackward>)\n",
      "tensor(0.0137, grad_fn=<NllLossBackward>)\n",
      "tensor(0.0137, grad_fn=<NllLossBackward>)\n",
      "tensor(0.0137, grad_fn=<NllLossBackward>)\n",
      "tensor(0.0137, grad_fn=<NllLossBackward>)\n",
      "tensor(0.0137, grad_fn=<NllLossBackward>)\n",
      "tensor(0.0137, grad_fn=<NllLossBackward>)\n",
      "tensor(0.0136, grad_fn=<NllLossBackward>)\n",
      "tensor(0.0136, grad_fn=<NllLossBackward>)\n",
      "tensor(0.0136, grad_fn=<NllLossBackward>)\n",
      "tensor(0.0136, grad_fn=<NllLossBackward>)\n",
      "tensor(0.0136, grad_fn=<NllLossBackward>)\n",
      "tensor(0.0135, grad_fn=<NllLossBackward>)\n",
      "tensor(0.0135, grad_fn=<NllLossBackward>)\n",
      "tensor(0.0135, grad_fn=<NllLossBackward>)\n",
      "tensor(0.0135, grad_fn=<NllLossBackward>)\n",
      "tensor(0.0135, grad_fn=<NllLossBackward>)\n",
      "tensor(0.0135, grad_fn=<NllLossBackward>)\n",
      "tensor(0.0134, grad_fn=<NllLossBackward>)\n",
      "tensor(0.0134, grad_fn=<NllLossBackward>)\n",
      "tensor(0.0134, grad_fn=<NllLossBackward>)\n",
      "tensor(0.0134, grad_fn=<NllLossBackward>)\n",
      "tensor(0.0134, grad_fn=<NllLossBackward>)\n",
      "tensor(0.0133, grad_fn=<NllLossBackward>)\n",
      "tensor(0.0133, grad_fn=<NllLossBackward>)\n",
      "tensor(0.0133, grad_fn=<NllLossBackward>)\n",
      "tensor(0.0133, grad_fn=<NllLossBackward>)\n",
      "tensor(0.0133, grad_fn=<NllLossBackward>)\n",
      "tensor(0.0133, grad_fn=<NllLossBackward>)\n",
      "tensor(0.0132, grad_fn=<NllLossBackward>)\n",
      "tensor(0.0132, grad_fn=<NllLossBackward>)\n",
      "tensor(0.0132, grad_fn=<NllLossBackward>)\n",
      "tensor(0.0132, grad_fn=<NllLossBackward>)\n",
      "tensor(0.0132, grad_fn=<NllLossBackward>)\n",
      "tensor(0.0132, grad_fn=<NllLossBackward>)\n",
      "tensor(0.0131, grad_fn=<NllLossBackward>)\n",
      "tensor(0.0131, grad_fn=<NllLossBackward>)\n",
      "tensor(0.0131, grad_fn=<NllLossBackward>)\n",
      "tensor(0.0131, grad_fn=<NllLossBackward>)\n",
      "tensor(0.0131, grad_fn=<NllLossBackward>)\n",
      "tensor(0.0131, grad_fn=<NllLossBackward>)\n",
      "tensor(0.0130, grad_fn=<NllLossBackward>)\n",
      "tensor(0.0130, grad_fn=<NllLossBackward>)\n",
      "tensor(0.0130, grad_fn=<NllLossBackward>)\n",
      "tensor(0.0130, grad_fn=<NllLossBackward>)\n",
      "tensor(0.0130, grad_fn=<NllLossBackward>)\n",
      "tensor(0.0130, grad_fn=<NllLossBackward>)\n",
      "tensor(0.0129, grad_fn=<NllLossBackward>)\n",
      "tensor(0.0129, grad_fn=<NllLossBackward>)\n",
      "tensor(0.0129, grad_fn=<NllLossBackward>)\n",
      "tensor(0.0129, grad_fn=<NllLossBackward>)\n",
      "tensor(0.0129, grad_fn=<NllLossBackward>)\n",
      "tensor(0.0129, grad_fn=<NllLossBackward>)\n",
      "tensor(0.0128, grad_fn=<NllLossBackward>)\n",
      "tensor(0.0128, grad_fn=<NllLossBackward>)\n",
      "tensor(0.0128, grad_fn=<NllLossBackward>)\n",
      "tensor(0.0128, grad_fn=<NllLossBackward>)\n",
      "tensor(0.0128, grad_fn=<NllLossBackward>)\n",
      "tensor(0.0128, grad_fn=<NllLossBackward>)\n",
      "tensor(0.0128, grad_fn=<NllLossBackward>)\n",
      "tensor(0.0127, grad_fn=<NllLossBackward>)\n",
      "tensor(0.0127, grad_fn=<NllLossBackward>)\n",
      "tensor(0.0127, grad_fn=<NllLossBackward>)\n",
      "tensor(0.0127, grad_fn=<NllLossBackward>)\n",
      "tensor(0.0127, grad_fn=<NllLossBackward>)\n",
      "tensor(0.0127, grad_fn=<NllLossBackward>)\n",
      "tensor(0.0126, grad_fn=<NllLossBackward>)\n",
      "tensor(0.0126, grad_fn=<NllLossBackward>)\n",
      "tensor(0.0126, grad_fn=<NllLossBackward>)\n",
      "tensor(0.0126, grad_fn=<NllLossBackward>)\n",
      "tensor(0.0126, grad_fn=<NllLossBackward>)\n",
      "tensor(0.0126, grad_fn=<NllLossBackward>)\n",
      "tensor(0.0125, grad_fn=<NllLossBackward>)\n",
      "tensor(0.0125, grad_fn=<NllLossBackward>)\n",
      "tensor(0.0125, grad_fn=<NllLossBackward>)\n",
      "tensor(0.0125, grad_fn=<NllLossBackward>)\n",
      "tensor(0.0125, grad_fn=<NllLossBackward>)\n",
      "tensor(0.0125, grad_fn=<NllLossBackward>)\n",
      "tensor(0.0125, grad_fn=<NllLossBackward>)\n",
      "tensor(0.0124, grad_fn=<NllLossBackward>)\n",
      "tensor(0.0124, grad_fn=<NllLossBackward>)\n",
      "tensor(0.0124, grad_fn=<NllLossBackward>)\n",
      "tensor(0.0124, grad_fn=<NllLossBackward>)\n",
      "tensor(0.0124, grad_fn=<NllLossBackward>)\n",
      "tensor(0.0124, grad_fn=<NllLossBackward>)\n",
      "tensor(0.0124, grad_fn=<NllLossBackward>)\n",
      "tensor(0.0123, grad_fn=<NllLossBackward>)\n",
      "tensor(0.0123, grad_fn=<NllLossBackward>)\n",
      "tensor(0.0123, grad_fn=<NllLossBackward>)\n",
      "tensor(0.0123, grad_fn=<NllLossBackward>)\n",
      "tensor(0.0123, grad_fn=<NllLossBackward>)\n",
      "tensor(0.0123, grad_fn=<NllLossBackward>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.0123, grad_fn=<NllLossBackward>)\n",
      "tensor(0.0122, grad_fn=<NllLossBackward>)\n",
      "tensor(0.0122, grad_fn=<NllLossBackward>)\n",
      "tensor(0.0122, grad_fn=<NllLossBackward>)\n",
      "tensor(0.0122, grad_fn=<NllLossBackward>)\n",
      "tensor(0.0122, grad_fn=<NllLossBackward>)\n",
      "tensor(0.0122, grad_fn=<NllLossBackward>)\n",
      "tensor(0.0122, grad_fn=<NllLossBackward>)\n",
      "tensor(0.0121, grad_fn=<NllLossBackward>)\n",
      "tensor(0.0121, grad_fn=<NllLossBackward>)\n",
      "tensor(0.0121, grad_fn=<NllLossBackward>)\n",
      "tensor(0.0121, grad_fn=<NllLossBackward>)\n",
      "tensor(0.0121, grad_fn=<NllLossBackward>)\n",
      "tensor(0.0121, grad_fn=<NllLossBackward>)\n",
      "tensor(0.0121, grad_fn=<NllLossBackward>)\n",
      "tensor(0.0120, grad_fn=<NllLossBackward>)\n",
      "tensor(0.0120, grad_fn=<NllLossBackward>)\n",
      "tensor(0.0120, grad_fn=<NllLossBackward>)\n",
      "tensor(0.0120, grad_fn=<NllLossBackward>)\n",
      "tensor(0.0120, grad_fn=<NllLossBackward>)\n",
      "tensor(0.0120, grad_fn=<NllLossBackward>)\n",
      "tensor(0.0120, grad_fn=<NllLossBackward>)\n",
      "tensor(0.0119, grad_fn=<NllLossBackward>)\n",
      "tensor(0.0119, grad_fn=<NllLossBackward>)\n",
      "tensor(0.0119, grad_fn=<NllLossBackward>)\n",
      "tensor(0.0119, grad_fn=<NllLossBackward>)\n",
      "tensor(0.0119, grad_fn=<NllLossBackward>)\n",
      "tensor(0.0119, grad_fn=<NllLossBackward>)\n",
      "tensor(0.0119, grad_fn=<NllLossBackward>)\n",
      "tensor(0.0118, grad_fn=<NllLossBackward>)\n",
      "tensor(0.0118, grad_fn=<NllLossBackward>)\n",
      "tensor(0.0118, grad_fn=<NllLossBackward>)\n",
      "tensor(0.0118, grad_fn=<NllLossBackward>)\n",
      "tensor(0.0118, grad_fn=<NllLossBackward>)\n",
      "tensor(0.0118, grad_fn=<NllLossBackward>)\n",
      "tensor(0.0118, grad_fn=<NllLossBackward>)\n",
      "tensor(0.0117, grad_fn=<NllLossBackward>)\n",
      "tensor(0.0117, grad_fn=<NllLossBackward>)\n",
      "tensor(0.0117, grad_fn=<NllLossBackward>)\n",
      "tensor(0.0117, grad_fn=<NllLossBackward>)\n",
      "tensor(0.0117, grad_fn=<NllLossBackward>)\n",
      "tensor(0.0117, grad_fn=<NllLossBackward>)\n",
      "tensor(0.0117, grad_fn=<NllLossBackward>)\n",
      "tensor(0.0117, grad_fn=<NllLossBackward>)\n",
      "tensor(0.0116, grad_fn=<NllLossBackward>)\n",
      "tensor(0.0116, grad_fn=<NllLossBackward>)\n",
      "tensor(0.0116, grad_fn=<NllLossBackward>)\n",
      "tensor(0.0116, grad_fn=<NllLossBackward>)\n",
      "tensor(0.0116, grad_fn=<NllLossBackward>)\n",
      "tensor(0.0116, grad_fn=<NllLossBackward>)\n",
      "tensor(0.0116, grad_fn=<NllLossBackward>)\n",
      "tensor(0.0116, grad_fn=<NllLossBackward>)\n",
      "tensor(0.0115, grad_fn=<NllLossBackward>)\n",
      "tensor(0.0115, grad_fn=<NllLossBackward>)\n",
      "tensor(0.0115, grad_fn=<NllLossBackward>)\n",
      "tensor(0.0115, grad_fn=<NllLossBackward>)\n",
      "tensor(0.0115, grad_fn=<NllLossBackward>)\n",
      "tensor(0.0115, grad_fn=<NllLossBackward>)\n",
      "tensor(0.0115, grad_fn=<NllLossBackward>)\n",
      "tensor(0.0115, grad_fn=<NllLossBackward>)\n",
      "tensor(0.0114, grad_fn=<NllLossBackward>)\n",
      "tensor(0.0114, grad_fn=<NllLossBackward>)\n",
      "tensor(0.0114, grad_fn=<NllLossBackward>)\n",
      "tensor(0.0114, grad_fn=<NllLossBackward>)\n",
      "tensor(0.0114, grad_fn=<NllLossBackward>)\n",
      "tensor(0.0114, grad_fn=<NllLossBackward>)\n",
      "tensor(0.0114, grad_fn=<NllLossBackward>)\n",
      "tensor(0.0114, grad_fn=<NllLossBackward>)\n",
      "tensor(0.0113, grad_fn=<NllLossBackward>)\n",
      "tensor(0.0113, grad_fn=<NllLossBackward>)\n",
      "tensor(0.0113, grad_fn=<NllLossBackward>)\n",
      "tensor(0.0113, grad_fn=<NllLossBackward>)\n",
      "tensor(0.0113, grad_fn=<NllLossBackward>)\n",
      "tensor(0.0113, grad_fn=<NllLossBackward>)\n",
      "tensor(0.0113, grad_fn=<NllLossBackward>)\n",
      "tensor(0.0113, grad_fn=<NllLossBackward>)\n",
      "tensor(0.0112, grad_fn=<NllLossBackward>)\n",
      "tensor(0.0112, grad_fn=<NllLossBackward>)\n",
      "tensor(0.0112, grad_fn=<NllLossBackward>)\n",
      "tensor(0.0112, grad_fn=<NllLossBackward>)\n",
      "tensor(0.0112, grad_fn=<NllLossBackward>)\n",
      "tensor(0.0112, grad_fn=<NllLossBackward>)\n",
      "tensor(0.0112, grad_fn=<NllLossBackward>)\n",
      "tensor(0.0112, grad_fn=<NllLossBackward>)\n",
      "tensor(0.0111, grad_fn=<NllLossBackward>)\n",
      "tensor(0.0111, grad_fn=<NllLossBackward>)\n",
      "tensor(0.0111, grad_fn=<NllLossBackward>)\n",
      "tensor(0.0111, grad_fn=<NllLossBackward>)\n",
      "tensor(0.0111, grad_fn=<NllLossBackward>)\n",
      "tensor(0.0111, grad_fn=<NllLossBackward>)\n",
      "tensor(0.0111, grad_fn=<NllLossBackward>)\n",
      "tensor(0.0111, grad_fn=<NllLossBackward>)\n",
      "tensor(0.0110, grad_fn=<NllLossBackward>)\n",
      "tensor(0.0110, grad_fn=<NllLossBackward>)\n",
      "tensor(0.0110, grad_fn=<NllLossBackward>)\n",
      "tensor(0.0110, grad_fn=<NllLossBackward>)\n",
      "tensor(0.0110, grad_fn=<NllLossBackward>)\n",
      "tensor(0.0110, grad_fn=<NllLossBackward>)\n",
      "tensor(0.0110, grad_fn=<NllLossBackward>)\n",
      "tensor(0.0110, grad_fn=<NllLossBackward>)\n",
      "tensor(0.0110, grad_fn=<NllLossBackward>)\n",
      "tensor(0.0109, grad_fn=<NllLossBackward>)\n",
      "tensor(0.0109, grad_fn=<NllLossBackward>)\n",
      "tensor(0.0109, grad_fn=<NllLossBackward>)\n",
      "tensor(0.0109, grad_fn=<NllLossBackward>)\n",
      "tensor(0.0109, grad_fn=<NllLossBackward>)\n",
      "tensor(0.0109, grad_fn=<NllLossBackward>)\n",
      "tensor(0.0109, grad_fn=<NllLossBackward>)\n",
      "tensor(0.0109, grad_fn=<NllLossBackward>)\n",
      "tensor(0.0109, grad_fn=<NllLossBackward>)\n",
      "tensor(0.0108, grad_fn=<NllLossBackward>)\n",
      "tensor(0.0108, grad_fn=<NllLossBackward>)\n",
      "tensor(0.0108, grad_fn=<NllLossBackward>)\n",
      "tensor(0.0108, grad_fn=<NllLossBackward>)\n",
      "tensor(0.0108, grad_fn=<NllLossBackward>)\n",
      "tensor(0.0108, grad_fn=<NllLossBackward>)\n",
      "tensor(0.0108, grad_fn=<NllLossBackward>)\n",
      "tensor(0.0108, grad_fn=<NllLossBackward>)\n",
      "tensor(0.0108, grad_fn=<NllLossBackward>)\n",
      "tensor(0.0107, grad_fn=<NllLossBackward>)\n",
      "tensor(0.0107, grad_fn=<NllLossBackward>)\n",
      "tensor(0.0107, grad_fn=<NllLossBackward>)\n",
      "tensor(0.0107, grad_fn=<NllLossBackward>)\n",
      "tensor(0.0107, grad_fn=<NllLossBackward>)\n",
      "tensor(0.0107, grad_fn=<NllLossBackward>)\n",
      "tensor(0.0107, grad_fn=<NllLossBackward>)\n",
      "tensor(0.0107, grad_fn=<NllLossBackward>)\n",
      "tensor(0.0107, grad_fn=<NllLossBackward>)\n",
      "tensor(0.0106, grad_fn=<NllLossBackward>)\n",
      "tensor(0.0106, grad_fn=<NllLossBackward>)\n",
      "tensor(0.0106, grad_fn=<NllLossBackward>)\n",
      "tensor(0.0106, grad_fn=<NllLossBackward>)\n",
      "tensor(0.0106, grad_fn=<NllLossBackward>)\n",
      "tensor(0.0106, grad_fn=<NllLossBackward>)\n",
      "tensor(0.0106, grad_fn=<NllLossBackward>)\n",
      "tensor(0.0106, grad_fn=<NllLossBackward>)\n",
      "tensor(0.0106, grad_fn=<NllLossBackward>)\n",
      "tensor(0.0105, grad_fn=<NllLossBackward>)\n",
      "tensor(0.0105, grad_fn=<NllLossBackward>)\n",
      "tensor(0.0105, grad_fn=<NllLossBackward>)\n",
      "tensor(0.0105, grad_fn=<NllLossBackward>)\n",
      "tensor(0.0105, grad_fn=<NllLossBackward>)\n",
      "tensor(0.0105, grad_fn=<NllLossBackward>)\n",
      "tensor(0.0105, grad_fn=<NllLossBackward>)\n",
      "tensor(0.0105, grad_fn=<NllLossBackward>)\n",
      "tensor(0.0105, grad_fn=<NllLossBackward>)\n",
      "tensor(0.0105, grad_fn=<NllLossBackward>)\n",
      "tensor(0.0104, grad_fn=<NllLossBackward>)\n",
      "tensor(0.0104, grad_fn=<NllLossBackward>)\n",
      "tensor(0.0104, grad_fn=<NllLossBackward>)\n",
      "tensor(0.0104, grad_fn=<NllLossBackward>)\n",
      "tensor(0.0104, grad_fn=<NllLossBackward>)\n",
      "tensor(0.0104, grad_fn=<NllLossBackward>)\n",
      "tensor(0.0104, grad_fn=<NllLossBackward>)\n",
      "tensor(0.0104, grad_fn=<NllLossBackward>)\n",
      "tensor(0.0104, grad_fn=<NllLossBackward>)\n",
      "tensor(0.0103, grad_fn=<NllLossBackward>)\n",
      "tensor(0.0103, grad_fn=<NllLossBackward>)\n",
      "tensor(0.0103, grad_fn=<NllLossBackward>)\n",
      "tensor(0.0103, grad_fn=<NllLossBackward>)\n",
      "tensor(0.0103, grad_fn=<NllLossBackward>)\n",
      "tensor(0.0103, grad_fn=<NllLossBackward>)\n",
      "tensor(0.0103, grad_fn=<NllLossBackward>)\n",
      "tensor(0.0103, grad_fn=<NllLossBackward>)\n",
      "tensor(0.0103, grad_fn=<NllLossBackward>)\n",
      "tensor(0.0103, grad_fn=<NllLossBackward>)\n",
      "tensor(0.0102, grad_fn=<NllLossBackward>)\n",
      "tensor(0.0102, grad_fn=<NllLossBackward>)\n",
      "tensor(0.0102, grad_fn=<NllLossBackward>)\n",
      "tensor(0.0102, grad_fn=<NllLossBackward>)\n",
      "tensor(0.0102, grad_fn=<NllLossBackward>)\n",
      "tensor(0.0102, grad_fn=<NllLossBackward>)\n",
      "tensor(0.0102, grad_fn=<NllLossBackward>)\n",
      "tensor(0.0102, grad_fn=<NllLossBackward>)\n",
      "tensor(0.0102, grad_fn=<NllLossBackward>)\n",
      "tensor(0.0102, grad_fn=<NllLossBackward>)\n",
      "tensor(0.0101, grad_fn=<NllLossBackward>)\n",
      "tensor(0.0101, grad_fn=<NllLossBackward>)\n",
      "tensor(0.0101, grad_fn=<NllLossBackward>)\n",
      "tensor(0.0101, grad_fn=<NllLossBackward>)\n",
      "tensor(0.0101, grad_fn=<NllLossBackward>)\n",
      "tensor(0.0101, grad_fn=<NllLossBackward>)\n",
      "tensor(0.0101, grad_fn=<NllLossBackward>)\n",
      "tensor(0.0101, grad_fn=<NllLossBackward>)\n",
      "tensor(0.0101, grad_fn=<NllLossBackward>)\n",
      "tensor(0.0101, grad_fn=<NllLossBackward>)\n",
      "tensor(0.0101, grad_fn=<NllLossBackward>)\n",
      "tensor(0.0100, grad_fn=<NllLossBackward>)\n",
      "tensor(0.0100, grad_fn=<NllLossBackward>)\n",
      "tensor(0.0100, grad_fn=<NllLossBackward>)\n",
      "tensor(0.0100, grad_fn=<NllLossBackward>)\n",
      "tensor(0.0100, grad_fn=<NllLossBackward>)\n",
      "tensor(0.0100, grad_fn=<NllLossBackward>)\n",
      "tensor(0.0100, grad_fn=<NllLossBackward>)\n",
      "tensor(0.0100, grad_fn=<NllLossBackward>)\n",
      "tensor(0.0100, grad_fn=<NllLossBackward>)\n",
      "tensor(0.0100, grad_fn=<NllLossBackward>)\n",
      "tensor(0.0099, grad_fn=<NllLossBackward>)\n",
      "tensor(0.0099, grad_fn=<NllLossBackward>)\n",
      "tensor(0.0099, grad_fn=<NllLossBackward>)\n",
      "tensor(0.0099, grad_fn=<NllLossBackward>)\n",
      "tensor(0.0099, grad_fn=<NllLossBackward>)\n",
      "tensor(0.0099, grad_fn=<NllLossBackward>)\n",
      "tensor(0.0099, grad_fn=<NllLossBackward>)\n",
      "tensor(0.0099, grad_fn=<NllLossBackward>)\n",
      "tensor(0.0099, grad_fn=<NllLossBackward>)\n",
      "tensor(0.0099, grad_fn=<NllLossBackward>)\n",
      "tensor(0.0099, grad_fn=<NllLossBackward>)\n",
      "tensor(0.0098, grad_fn=<NllLossBackward>)\n",
      "tensor(0.0098, grad_fn=<NllLossBackward>)\n",
      "tensor(0.0098, grad_fn=<NllLossBackward>)\n",
      "tensor(0.0098, grad_fn=<NllLossBackward>)\n",
      "tensor(0.0098, grad_fn=<NllLossBackward>)\n",
      "tensor(0.0098, grad_fn=<NllLossBackward>)\n",
      "tensor(0.0098, grad_fn=<NllLossBackward>)\n",
      "tensor(0.0098, grad_fn=<NllLossBackward>)\n",
      "tensor(0.0098, grad_fn=<NllLossBackward>)\n",
      "tensor(0.0098, grad_fn=<NllLossBackward>)\n",
      "tensor(0.0098, grad_fn=<NllLossBackward>)\n",
      "tensor(0.0097, grad_fn=<NllLossBackward>)\n",
      "tensor(0.0097, grad_fn=<NllLossBackward>)\n",
      "tensor(0.0097, grad_fn=<NllLossBackward>)\n",
      "tensor(0.0097, grad_fn=<NllLossBackward>)\n",
      "tensor(0.0097, grad_fn=<NllLossBackward>)\n",
      "tensor(0.0097, grad_fn=<NllLossBackward>)\n",
      "tensor(0.0097, grad_fn=<NllLossBackward>)\n",
      "tensor(0.0097, grad_fn=<NllLossBackward>)\n",
      "tensor(0.0097, grad_fn=<NllLossBackward>)\n",
      "tensor(0.0097, grad_fn=<NllLossBackward>)\n",
      "tensor(0.0097, grad_fn=<NllLossBackward>)\n",
      "tensor(0.0096, grad_fn=<NllLossBackward>)\n",
      "tensor(0.0096, grad_fn=<NllLossBackward>)\n",
      "tensor(0.0096, grad_fn=<NllLossBackward>)\n",
      "tensor(0.0096, grad_fn=<NllLossBackward>)\n",
      "tensor(0.0096, grad_fn=<NllLossBackward>)\n",
      "tensor(0.0096, grad_fn=<NllLossBackward>)\n",
      "tensor(0.0096, grad_fn=<NllLossBackward>)\n",
      "tensor(0.0096, grad_fn=<NllLossBackward>)\n"
     ]
    }
   ],
   "source": [
    "# In a training loop, we should perform many GD iterations.\n",
    "nIter = 1000\n",
    "for i in range(nIter):\n",
    "    optimizer.zero_grad() # equivalent to net.zero_grad()\n",
    "    output = net(x)\n",
    "    loss = criterion(output, y)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    print(loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Saving and Loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "odict_keys(['0.weight', '0.bias', '2.weight', '2.bias'])\n"
     ]
    }
   ],
   "source": [
    "# get dictionary of keys to weights using 'state_dict'\n",
    "net = torch.nn.Sequential(\n",
    "    torch.nn.Linear(28*28, 256),\n",
    "    torch.nn.Sigmoid(),\n",
    "    torch.nn.Linear(256, 10)\n",
    ")\n",
    "print(net.state_dict().keys())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# save a dictionary\n",
    "torch.save(net.state_dict(), 'test.t10')\n",
    "# load a dictionary\n",
    "net.load_state_dict(torch.load('test.t10'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Type mismatch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "Expected object of scalar type Long but got scalar type Float for argument #3 'mat2' in call to _th_addmm_out",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-26-e84ccf4a44b8>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mnet\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mLinear\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m4\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m4\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnet\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/trchenv3.7/lib/python3.7/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    720\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    721\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 722\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    723\u001b[0m         for hook in itertools.chain(\n\u001b[1;32m    724\u001b[0m                 \u001b[0m_global_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/trchenv3.7/lib/python3.7/site-packages/torch/nn/modules/linear.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m     89\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     90\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 91\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlinear\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbias\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     92\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     93\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mextra_repr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/trchenv3.7/lib/python3.7/site-packages/torch/nn/functional.py\u001b[0m in \u001b[0;36mlinear\u001b[0;34m(input, weight, bias)\u001b[0m\n\u001b[1;32m   1674\u001b[0m         \u001b[0mret\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maddmm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbias\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweight\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1675\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1676\u001b[0;31m         \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmatmul\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1677\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mbias\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1678\u001b[0m             \u001b[0moutput\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mbias\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Expected object of scalar type Long but got scalar type Float for argument #3 'mat2' in call to _th_addmm_out"
     ]
    }
   ],
   "source": [
    "net = nn.Linear(4, 2)\n",
    "x = torch.tensor([1,2,3,4])\n",
    "y = net(x)\n",
    "print(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([ 0.5646, -0.3501], grad_fn=<AddBackward0>)\n",
      "tensor([[2., 2.],\n",
      "        [2., 2.]])\n",
      "tensor([[3., 3.],\n",
      "        [3., 3.]])\n",
      "tensor([[6., 6.],\n",
      "        [6., 6.]])\n",
      "tensor([[12., 12.],\n",
      "        [12., 12.]])\n"
     ]
    }
   ],
   "source": [
    "x = x.float()\n",
    "x = torch.tensor([1.,2.,3.,4.])\n",
    "y = net(x)\n",
    "print(y)\n",
    "x = 2 * torch.ones(2, 2)\n",
    "y = 3 * torch.ones(2, 2)\n",
    "print(x)\n",
    "print(y)\n",
    "print(x * y)\n",
    "print(x.matmul(y))\n",
    "# print(x.dot(y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1., 1., 1., 1., 1.],\n",
      "        [1., 1., 1., 1., 1.],\n",
      "        [1., 1., 1., 1., 1.],\n",
      "        [1., 1., 1., 1., 1.]])\n",
      "tensor([0, 1, 2, 3, 4])\n",
      "tensor([[1., 2., 3., 4., 5.],\n",
      "        [1., 2., 3., 4., 5.],\n",
      "        [1., 2., 3., 4., 5.],\n",
      "        [1., 2., 3., 4., 5.]])\n",
      "tensor([[0],\n",
      "        [1],\n",
      "        [2],\n",
      "        [3]])\n",
      "tensor([[1., 1., 1., 1., 1.],\n",
      "        [2., 2., 2., 2., 2.],\n",
      "        [3., 3., 3., 3., 3.],\n",
      "        [4., 4., 4., 4., 4.]])\n",
      "tensor([0, 1, 2, 3])\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "The size of tensor a (5) must match the size of tensor b (4) at non-singleton dimension 1",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-29-636533761f18>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m4\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m: The size of tensor a (5) must match the size of tensor b (4) at non-singleton dimension 1"
     ]
    }
   ],
   "source": [
    "x = torch.ones(4, 5)\n",
    "y = torch.arange(5)\n",
    "print(x)\n",
    "print(y)\n",
    "print(x+y)\n",
    "y = torch.arange(4).view(-1,1)\n",
    "print(y)\n",
    "print(x+y)\n",
    "y = torch.arange(4)\n",
    "print(y)\n",
    "print(x+y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1, 2, 3],\n",
      "        [4, 5, 6]])\n",
      "tensor([[1, 4],\n",
      "        [2, 5],\n",
      "        [3, 6]])\n",
      "tensor([[1, 2],\n",
      "        [3, 4],\n",
      "        [5, 6]])\n",
      "tensor([[1, 2, 3],\n",
      "        [4, 5, 6]])\n"
     ]
    }
   ],
   "source": [
    "x = torch.tensor([[1,2,3], [4,5,6]])\n",
    "print(x)\n",
    "print(x.t())\n",
    "print(x.view(3,2))\n",
    "print(x.view(2,3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class MyNet(nn.Module):\n",
    "    def __init__(self, nHiddenLayers):\n",
    "        super(MyNet, self).__init__()\n",
    "        self.nHiddenLayers=nHiddenLayers\n",
    "        self.finalLayer = nn.Linear(128, 10)\n",
    "        self.act = nn.ReLU()\n",
    "        self.hidden = []\n",
    "        for i in range(nHiddenLayers):\n",
    "            self.hidden.append(nn.Linear(128, 128))\n",
    "    \n",
    "    def forward(self, x):\n",
    "        h = x\n",
    "        for i in range(self.nHiddenLayers):\n",
    "            h = self.hidden[i](h)\n",
    "            h = self.act(h)\n",
    "        out = self.finalLayer(h)\n",
    "        return out"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
